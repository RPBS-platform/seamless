Ideas that are superfluous. Keep them here, just in case...

- No more registrars from 0.1. They have been ABANDONED. Same for signals.

- Terminology: context children can be private, in which case they are not in __dir__.
  By default, they are public. No more "export".
  (In general, look at __dir__ in the context of Silk, mixed, and cells)
  (NOTE: This has nothing to do with services. Services determine private and public based on
  what connects into the serviced context.)

- Mid-level: There will be a special library contexts of low-level macros that recognize mid-level graph constructs
  and return low-level contexts. UPDATE: just macros should do
  These contexts are expected to accept connections straightforwardly, and to have hook registration configured.
  The "big low-level macro" (only invoked by the top context) reads in a mid-level graph + such a library,
  and returns a big low-level context (mid-to-low-level translation).

- Connection layers: depend on one or multiple contexts (and other inputs) and a code cell.
  Code cell is executed as if a macro, but can only add connections.
  Whenever one connection layer of a context becomes dirty, all of them become dirty. Other input contexts also become dirty in their connection layers.
  UPDATE: no need; static connection layer + dynamic connection layer that has explicit context inputs
- Renaming is very hard to cache. So the low-level macro cache function can receive (from the high level) a renaming key.
  This renaming key changes the current (sub-)context against which the renamed context is evaluated.
  Normally, every renaming triggers a mid-level-to-low-level translation, so there should be only renaming key.
  UPDATE: don't cache this. Cache code cell hash + input hashes + transformer/reactor/macro declaration dict
- If you .set() a cell and then connect to it (making it non-authoritative), a warning is printed.
  Use .set_default() to avoid this warning.
- There will be a runtime API to control the low-level from the mid-level. This is just another kind of caching; the low-level could as well be regenerated.
  The runtime API can be done automatically, if all mid-level-to-low-level core macros defines cell-to-cell correspondences. These core macros understand all mid-level nodes. It will "just work" for embedded low-level macro nodes too.
  Authoritative cells can be set directly from the mid-level. Setting non-authoritative cells will generate the usual warning. Setting cells that control (low-level) macros will re-execute them (with proper caching).
  UPDATE: better to have the mid-level contain explicit pointers to low-level authoritative data. This way, modifying the low level
    auto-modifies the mid level.
- There will be an option to sync cells to a file. This must be done in macro mode.
  The sync can be one-way or two-way (only for authoritative cells). When a cell is updated from a file, it is as if it was done using .set()
  A context can be synced to a directory.
  Cell/context symlinks will become Unix file/directory symlinks.
  When a cell/context is destroyed, the file is cleaned up. This can be prevented by a low-level-macro-caching hit.


- Address GLstore memory leak: stores may not be freed (is this still so post-0.1 ??)
(Use Blocks instead)

- implement successors (YAGNI? now that the low-level is subordinate, better rip them)

- ATC chains with folding/unfolding (YAGNI?)

- Dynamic connection layers: a special macro that has one or more contexts as input (among other inputs), which must be (grand)children
   Like static layers, they are tied to a macro or toplevel context
   They take as input any children of the context (cells or child contexts);
   Builds connections within/between those children.
   They can never build new cells, use the help of a macro for that.
   The result layer consists of a set of pairs (connection + callback that removes that connection),
   Dynamic connection layers will always be evaluated after any other forward update.
   Therefore, if you have A => macro => B, A + B => dynamic layer, you can assume that the macro
    will respond first to changes in A, so that B will reflect the new A.

    - Report cells (JSON cells, can become structured if directed from the mid-level).
      Status dict becomes a a Report cell.
    - Log cell: text cell to which an observer can be attached that receives new entries)
       The result of translation, caching, macros, etc.; generalized log API.
       Even transformers and reactors may be declared as having a log output, and various loglevels
        (transformer.py will already send low-priority log messages about receiving events etc.)

Event streams
=============
Event streams receive event values, or a "undo" signal, which means that all previous
values are invalid. Event streams may send back a "send again" signal, which means
that they want again all values that were previously sent to them. (This is for example if a transformer adds 5 to an event stream; if 5 is changed to 6, either in a input cell or by a change in the source code, the transformer will send a "send again" signal upstream).
Reactors may choose to cache events so to avoid sending this signal when one of their other inputs changes.
Finally, there are the "initialize" and "disconnect" signals.
Event stream inputpins are in a state where they accept a new value, or they don't. Force-feeding is possible, in that case the values are buffered up by seamless itself.
Workers must always declare explicitly a pin as event stream.
Transformers don't need any change in their code. However, if one or more of their inputs is an event stream, so must be their output (vice versa is not required, but if all inputs are cells, a transformer that has an event stream as output must return a list, which is force-fed into the event stream after sending an "initialize" signal).
Reactors must push/pull new event stream values in an explicit API:
  - blocking waits for it
  - non-blocking essentially sets to event stream input pin to "accept input" (input) or force-feeds (output)
  - By default, input is blocking while output is non-blocking
Event streams can never be authoritative, they must depend on a worker.
NOTE: The trouble won't be the implementation... but event streams have serious consequences for caching!
...
(In relation to ATTRACT mainloop)
After event streams, the mainloop reactor will be superfluous
 and can be built as a simple context of event stream cell A-D plus connections,
 with an internal counter (event streams must provide this as a Report cell!) to make the B=>C connection conditional.
Equilibrium contexts with an event stream as input will automatically give one as output, and send the proper "undo" signal
 when they change, resetting the stream.

Compressed blocks
=================
If the mode is "compress", the block is tiled *physically* over "compress" tiles, but *logically* over "length" tiles.
For example, take a block of a million elements, with "length" as 1000 but "compress" as 10. This means that physically,
 the block is divided into 10 tiles of 100k elements, but *logically*, there exists 1000 tiles of 100k elements, for a
 logical array size of 100 million elements. Of those 1000 tiles, only 10 may be physically held in memory.
....
If only "split" mode is used, tiling is just a way to process parts of the buffer independently.
However, if "compress" mode is used, the situation becomes more complicated.
Like event streams, a tiling outchannel then holds a lock, which is released when all (but not really all, see below)
of its downstream  dependencies are done with it. The number of locks is equal to "compress", the number of physical tiles.

Runtime caching
===============
New cell type: cache cell.
All (low-level) workers (transformers/reactors/macros) may take (up to) one pin of type "cache" as inputpin (not editpin).
If they take such a pin, they may raise a CacheError. This will clear the cache, and put the worker
in "CacheError" state. CacheErrors are meant to detect *stale* caches: workers are forbidden to raise CacheError if the cache is empty. (UPDATE: multiple cache inputs, but CacheError clears all of them)
It is understood that the content of cache cells *do not influence the result whatsoever*.
  - Dirty cache cells do not trigger re-evaluation of downstream workers, unless those are in "CacheError" state.
  - Cache cells alone may have multiple sources of authority, i.e. multiple outputpins/editpins connecting to them.
    (UPDATE: better not do this...)
Special transformers are "caching transformers", they have a "cache" cell as output.
Caching transformers are triggered when their input changes *or* their cache output is cleared.
Caching transformers alone can have multiple cache inputs, and have an API to clear them individually.
Cache clearing counts as a signal in seamless, which means that the subsequent triggering of the caching transformers
has the highest evaluation priority.
Workers in "CacheError" state are re-evaluated whenever their cache input changes.
If the cache input stays cleared, and the context is in equilibrium, they are nevertheless evaluated with
empty cache (and the CacheError state is removed).
UPDATE: It is nice to co-opt this mechanism so that transformers can store partial results, and continue.
This can be done using a macro around a transformer (can be triggered using mid-level syntax).
The transformer has a cache inputpin that is connected from a cache cell. The same cache cell is also connected to
the secondary outputpin (result_preliminary), this must be allowed. (UPDATE: or use a special cache edit pin? this example
won't need a cache transformer then)
The macro generates a cache transformer that clears the cache cell whenever any of the inputs (including the code) changes.
The transformer code must be able to analyze the contents of the preliminary results in cache and act accordingly.
The cache cell will be marked as being serialized upon save (but not mounted).
UPDATE: Maybe re-think this a bit... maybe mark cells as "cache", and allow transformer edit pins only to "cache" cells.
 For the rest, rely on concretification signals to compute caches just in time (PIN.cache.value could trigger concretify
 in a blocking manner, no more CacheError foo?).

Network services (high level)
=============================
UPDATE: slightly outdated. Will be mostly implemented as high-level macros.
NOTE: these are to implement foreign (non-Seamless) web services. Seamless web services have better ways to communicate
 (see below).
Seamless will have the core concept of *network services*.
Seamless has a universal network service handler: it receives a protocol (REST, websocket, etc.),
a URI, a port, and JSON data. Data is sent, the result is returned.
Registering a network service takes the following parameters:
- type: can be "transformer", "reactor" or "macro"
- code: the code string that is serviced. The code string contains the source code of the transformer or macro. In case of "reactor", a dict of the three code strings. Also contains the language of the source code (default: Python)
- parameter pin dict. Must match the pin parameters of the transformer/reactor (equivalent for macro).
- adapter: code string (+ language) of the function that converts the input into parameters for the handler.
- schema_adapter: same, but receives the schema of the input instead (and also the code).
- handler_parameters: hard-coded parameters for the handler
- post_adapter: Another code string (+ language) to convert the handler results to pins. Optional for transformers/macros.
Adapter, schema_adapter must each return a dict, or raise an ServiceException if they decide that the service is not suitable based on the schema/the data.
The handler_parameters dict is updated by the schema_adapter result dict, then updated by the adapter result
dict, then sent to the handler. The result of the handler is passed to the post_adapter.
It is possible to set in the "evaluation" dict some flag that forces service evaluation: however, this
should not influence the result! The local code must be correct!

On top of this, network service macros can be implemented, that take slightly different parameters.
For example: named REST service handler, taking the following parameters:
- name: name of the REST service
- code: transformer code to be evaluated locally if the REST service is not found

Example: raw network service handler. Receives a URL + port + data. Sends data, returns the result.
Another: raw REST service handler. Same, but HTTP REST protocol.
Another: named network service handler. Receives not a URL but the *name* of a network service. Relies on a registry to convert this name into some kind of network call (could also be docker).
Remember that seamless assumes that the result of a computation is constant, regardless of service. So changing a service registry will not automatically re-evaluate the computation!
Now, the adapters also receive the "evaluation" parameter, so this can be forwarded to the handler!
Likewise, the adapters may combine this with its own "evaluation" analysis, based on what they receive.
For example, you may inform the ATTRACT grid computation service that your are planning to send 1 trillion
docking energy evaluations to the ATTRACT grid. A dumb ATTRACT grid service would build the grid on one machine,
and return some kind of session ID. This session ID is stored as cache in both the input and the result
"evaluation".
The session ID in the result "evaluation" can then be used to query the ATTRACT service with structures

The web publisher channels
===========================
Seamless will include a pocket web publisher. Each publisher can be made available on the Seamless router as a pair of RPCs: one to submit a web page under a path (providing some kind of
  authorization) and another to retrieve the page
- Static publisher: takes an HTML template and a host channel ID. The host channel ID is substituted into the HTML. The HTML is supposed to contact the channel via WAMP. If the channel comes
  from seamless, there will be a seamless collaborative sync protocol behind it: dynamic_html, or direct cell synchronization
  The static publisher does not take any arguments
- Dynamic publisher: takes an HTML template and a factory channel. The factory channel is invoked (without arguments) and returns a host channel ID.
A web server can serve the static publisher directly. The dynamic publisher should be accessible in two ways:
- Launcher: web page ID is in the request, no further arguments. Invokes the factory, redirects to the Retriever, with the host channel ID as parameter
- Retriever: web page ID in the request, host channel ID as parameter. Takes the HTML template, fills in the host channel and returns the HTML.
  As long as the host channel is open, the Retriever link will be universally accessible (no private browser connections).



  Macros
  High-level macros use the high-level API and generate a high-level context.
  (UPDATE: or a mid-level graph structure!)
  Low-level macros use the low-level (direct-mode) API and generate a low-level context.
    They can be embedded in the mid-level graph (specifying Python as the language)
  Some high-level contexts can be configured as *libraries*:
  - To be also available to any macro inside any child context of their parent, if the macro asks for it
  - To be also available as low-level Silk struct, inside low-level macros
  Such contexts will replace the standard library and registrars.
  UPDATE: libraries will be a low level concept now.
  All macros have four parts:
  - execution code
  - loading code (imports)
  - library requirements (see above)
  - configuration (language, what happens when you assign to it)
  Symlinks are also very important to tie a cell to a library cell. UPDATE: no longer true (LibCell instead)

- Organize cells into arrays (probably at high-level only)

- Other mount backends (databases)
  As a variation, an option for cells to have no .\_val, i.e. .value always pulls
   the value from the backend (it is assumed not to have changed!)
  Multiple mounts should be supported. Concept of "mount namespace", default ones: "file", "uri"
   Namespace is required, but mount identifier (file name, URL, ...) is optional (checksum might suffice)
   Mounts maybe read and/or write, but read may also be lazy: workers have to actively demand the value from
   the mount (this will not be kept in memory afterwards, caching has to be done by the mounter)
   UPDATE: maybe generic checksum value server will be enough?

 - Preliminary outputpins (in transformers [as secondary output] and in reactors)
 - Preliminary inputpins (pins that accept preliminary values). Right now, all inputpins are preliminary!
 - Equilibrium contexts (see below)
 - finalize the design of mid-level graph syntax.
   - Include old 0.1 resources, or make this high-level only?
   - Save high-level syntax as mid-level only, or separately?

 Concretification
 ================
 "Concretification" is a feature that cells may not need to store their full value.
 It involves sending two signals upstream, i.e. a cell's input pin or inchannels
 that requests something from the upstream worker or cell.
 1. send my hash/checksum
 2. (re-)send again my value
 Applications of this are in:
 - blocks (with compression; see below),
 - backend caching (a cell may just store its hash and demand its value when needed from
    a cell server; if the server fails, it may send a signal 2.)
 - lazy evaluation (in lazy mode, transformers evaluate only upon receiving a signal 2.
 Reactors and macros send a signal 2. to each of their input cells
 3. Release. In case of lazy cells, signal 2. is non-blocking; the manager is notified when
  the result is ready. The manager then forwards the results, and sends back a "release" signal,
  meaning that the value may be freed from memory. If this indeed happens or not, depends on
  the configuration (although compressed blocks always free).
  The checksum of the cell always kept. The value may still be in a checksum cell server.
  If the value cannot be retrieved, a signal 2. is sent upstream.
  (Non-lazy cells may also rely on such a server, but in their case, it is an error if the server fails.)

 Lazy evaluation
 ===============
 Lazy cells are cells that are marked as such. They must be dependent on a
  transformer.
 Reactors immediately concretify all of their lazy inputs, and cannot have a
  lazy output/edit.
 However, transformers that output lazy cells automatically become lazy.
 They concretify their inputs only when its output cell becomes concretified.

 An important advantage of lazy cells is that there are stronger timing guarantees.
 Signal 2. (invalidate the value) is propagated forward *immediately* by Seamless
  towards downstream dependencies (stopping transformers in their tracks), before
  sending any values to the workers.
 Example: dependency graph A=>B, A=>C, B+C=>D. If A is updated to A*,
  B will become updated to B* and C to C*.
  Eventually, D will be evaluated from B*+C*, but the intermediate computations
  B*+C or B+C* are both possible as "glitches". However, if B and C are lazy, then
  an update for A will immediately invalidate them, and the glitches disappear.
 From the Python shell, lazy cells have a method that concretifies in a non-blocking manner.
 In addition, they have a method that concretifies in a blocking manner
  and then returns a result.
 Finally, they have a channel that sends concretification signals,
  to which Python can subscribe callbacks (like an observer or Hive push antenna)

 In addition to LazyCell, there is LazyCallback, which emulates LazyCell.
 However, a LazyCallback wraps a zero-argument Python callback (blocking or non-blocking)
  into a cell value. This callback is triggered upon concretification.
 With cffi, the callback can come from another language, like C or even Haskell.
 Note that callback foo cannot be serialized, it is the responsibility of
  Python (or whatever host language) to bind the callbacks after graph instantiation.


Workers and memory
==================
COMPLETELY OUTDATED: Seamless now works already like this!
  (still some relevance for reactors. are discussed elsewhere)

By default, workers keep all of their values buffered. This may eat up a lot of
memory resources. Pins may be configured to be on-demand instead. In that case,
seamless will send only the checksums, and the worker will then have to ask for
the value.
In case of transformers/macros, this is a single request for all of the values.
For reactors, the request is actually made when the pin value is demanded.
This request is blocking, so it should probably only be used for async reactors.


Equilibrium contexts
====================
COMPLETELY OUTDATED: livegraph branch (0.2) makes it so that cell values are set *once*.
  There are no more glitches. Events no longer exist.

Transformers are guaranteed not to send anything (be it cell values or events) on their primary output until execution has finished (which means they are in equilibrium).
In addition, transformers are guaranteed not to accept any events while not in equilibrium. If there are any,
 the transformer computation is actually canceled.
This is obviously not so for reactors, and it is also not so for contexts that contain reactors (or multiple transformers that are not arranged linearly) connected to exported outputs of the context.
It is possible to declare contexts as "equilibrium contexts". In that case, they have the same guarantees as transformers have: sending cell updates or events to the outside world is delayed until equilibrium is reached, and so is the acceptance of new events. This allows contexts to perform atomic computations, reducing the number of glitches.
It is possible to declare some of the outputs as "secondary", which means that they escape this guarantee (for example, for logging purposes).
"Events to the outside world" is only restricted if it goes through exported cells and pins. Traffic through
non-exported objects is not restricted.

Block allocation
================
Block allocation can happen in two ways:
- Via an Allocator (new low-level construct). The block descriptor must be given to the allocator constructor.
  The Allocator provides an allocator output pin, which gives both the block descriptor and the allocated buffer.
- Via a StructuredCell, in Silk mode. The path of every allocation must be declared in "allocators", which works
  similar to "outchannels": each path becomes an allocator output pin.
  From the shell, the allocation path value is readable (resulting in the block value, copied to a Numpy array)
  but not writable. No inchannel or outchannel may be a superpath of the allocation path (at the high level, this
   must be checked during assignment, before translation).
  The StructuredCell must have all block descriptors declared during construction, including namespace.
  However, for high-level cells, the block descriptor is read from the .schema. In any case, if there is a schema
   entry, the StructuredCell will check that it is compatible with the block descriptor.
  When the OverlayMonitor (or its subclass) is created, a buffer is created (in the appropriate namespace)
  using the block descriptor, and when activated, both are sent over the allocator output pin.

  Application to ATTRACT: the mainloop reactor
  ============================================
  Because of the cylic dependency, an energy minimization loop is not a good fit for Seamless. But it can be done.
  There must be an ATTRACT mainloop reactor with editpins A and C, and inputpins B and D.
  Pin A contains the DOFs: upon start-up, it is copied from the initial DOFs.
  It is assumed that a "scorer" context listens to A and gives results on B, which is the energy and DOF gradients.
  To activate the scorer, the mainloop reactor sets A, then equilibrates A and B, then reads B.
  Pin C also contains the energy and DOF gradients, and D also contains the DOFs
  It is assumed that a "minimizer" context listens to C and then gives results on D.
  To activate the minimizer, the mainloop reactor sets C from B, then equilibrates C and D, then reads D.
  The scorer can then be re-activated by setting A from D.
  The minimizer and scorer contexts must be equilibrium contexts.
  "Equilibrating" is done by a having a signal that fires when a context reaches equilibrium.
   Use "on_equilibrate" hook of root managers (adapt for non-root).
  The ATTRACT mainloop reactor will have two signal inputpins for this (from scorer and minimizer) and an internal
   variable that maintains which signal is to be listened for.
  A and C are cached together with the number of minimization steps that have been performed. In this way:
  - The mainloop context can be saved in mid-evaluation and resumed later
  - The number of steps can be increased and only the additional steps are computed.
  Equilibrium contexts should also have a Report cell that reflects the checksum value of all exported cells and pins
  that are "input".
  Exported cells are "input" if they are authoritative within the cell. Exported pins are "input" if they are inputpins.
  The mainloop reactor will listen for these checksum Report cells; when they change, the whole computation must be restarted.
  When porting ATTRACT, the minimizer and scorer should be in slash.

  Control over side effect order (YAGNI: Observable notebooks are better than Hive)
  ==============================
  This can be done using Hive.
  - Cells are exposed to Hive as push inputpins. They work as observers.
    Hive push outputpins are exposed to Seamless as outputpins. This leads to a .set invocation
  - Alternatively, cells can be exposed as pull outputpins. The Hive pull leads to an .equilibrate + .value invocation.
    Hive pull outputpins can be exposed only via some DIY stream.
  - Signals can be exposed to Hive as push trigger inputpins, and vice versa.
  None of this uses any specific Hive<=>Seamless machinery, it all goes via Python callbacks.

- Non-deterministic outputpins. These are essentially editpins, except that
   the reactor is NOT notified if they are changed by some external source.

- Pure reactors
There is also a stricter formulation of purity: namely that B) and C) do not happen. Reactors and contexts may be marked "pure"
 accordingly. In that case, they function as a single caching unit.
Transformers must always be pure, no side effects allowed other than Report/Logging cells.
Impure reactors are not cachable and will always be executed (unless they are part of a pure context
 that gave a cache hit). Only impure reactors can take Report/Logging cells as inputpins. But you
  can always "cast" them to a normal cell (this normal cell will be authoritative).
Semi-pure reactors are not normally shut down, because of performance.

- Reactor execution.
  - Pure and semi-pure reactors can be executed async, just like transformers.
  - Because of this, they can be cached and submitted as remote jobs.
    Reactcache could be set up similar to transformcache.
    Better: generalize transformcache into async_cache to include pure reactors and graphs.
- Remote interactive mode execution.
  Semi-pure reactors can be remotely executed in interactive mode. Deltas are sent,
  and responses are received. The state itself must be labeled with an increasing ID,
  just as for shareserver. (Semi-)pure graphs can be executed in the same way.
- Graph remote execution. Only for pure graphs, i.e. that contain no impure reactors.
  They can be executed non-interactively (as if a transformer) or interactively
  (as if a semi-pure reactor).
- This is all orthogonal to collaborative mode, which deals with graphs but allows dynamic
  modification of the entire graph (whereas interactive mode is just the value of pins/cells).


4B: get BCsearch working
(initial/demo version works now)
- Follow roadmap (see BCsearch directory)
- Network evaluation:
  Almost done (tests/simple-remote-client/server)
  TODO:
  - embed in a real network server (and support it as .server, not just file://)
    UPDATE: this should be done by having a special GRAPH namespace on the REST server;
    the highlevel may share contexts (TODO) and transformers in this manner; sharing
    will expose the context/transformer to receive GRAPH commands; in a separate mechanism,
    a transformer/context can be configured in "delegated mode", meaning that local computation
    is disabled and that there must be either a cache hit or a worker that takes GRAPH commands)
  - send back the response (RPSEAMLESS?)
  - implement pin/manager.send_update_from_remote() that accepts the response
    and dispatches it towards the various cell types
  - make sure that a remote job_transformer loses equilibrium when a new result celltype
     is connected (changes output signature)
- Services proof-of-principle
  Low-level services (non-interactive transformer)

Plugin-socket connection system
===============================
(YAGNI, because at odds with reproducibility. In any case, the shareserver can do much of this.
Traitlets and observers can help, too).

Spooky effects at a distance.
Seamless will have a hive-like plugin-socket connection system to connect reactors.
These reactor connections are *not* meant to influence computations, but only to improve visualization
 and authority control.
Use cases:
- Have reactor 1 that polls a database, and reactor 2 that has an editpin to modify cell X.
  When reactor 1 and 2 are connected, cell X can be modified (as an act of authority) when the database
  is modified.
  Conversely, if reactor 2 receives updates to X, reactor 1 could store them in the database. For this,
  X could also come from an inputpin.
- Reactor 1 maintains an OpenGL window, and reactor 2 draws in it. When connected, reactor 1 can notify
  reactor 2 when the drawing can take place (since in OpenGL, drawing must take place within a specific
  callback triggered by the windowing system; With Vulkan, this may no longer be necessary).
- Reactor 1 and 2 both maintain a GUI using a native widget. By exposing their widget objects to reactor 3,
  reactor 3 can display them in a common window.
Plugins/sockets will be named by keys. You can hard-code the keys or send them as cells.
This system is obviously only for impure reactors. They are allowed to modify editpins as a result,
 but never outputpins.
Signals and callbacks will be implemented as part of the same system.

From an old roadmap... do we ever need interactive transformers???
==================================================================
  Low-level services (non-interactive transformer) will have been done
  Other possible services: interactive transformer, pure reactor (non-interactive or interactive)



  - High-level: pure contexts. Pure contexts have at least some public output cells and output pins.
  Only pure contexts are can have a grand computation result: the checksums of the public outputs are what is being
   stored as the result of the grand computation.
  Connections to private output cells/pins are not forbidden, but lead to a loss of purity (with a big warning),
   as does demanding their value from Python.
  Reactors and transformers can also be marked as pure (the high-level makes transformers pure by default).
  In both cases, all output pins are public.
  If they maintain their purity, a pure object is an independent caching unit. A hit in the
  computation checksum server will avoid the pure object to be (re-)translated at all.
  Also for the purpose of low-level caching, pure objects are a single dependency unit (a "virtual transformer"),
   so the low-level must be informed.
  UPDATE: everything is pure now


  - Signals. UPDATE: make them a special case of plugin-socket (see below). Probably delay this until long-term

- API to construct transformations without low-level contexts, launch them, and to check for cache hits.
  Application 1: to implement efficient non-deterministic execution that gives a deterministic result.
  Examples are: reduce or sort on deep cells that arrive piece-meal, or where the data may be near or far.
  Application 2: cyclic dependencies.