GPU
===

Various plans to use the GPU with Seamless.
NOTE: the GPU is and will be a can-of-worms for reproducibility
Part of this is the unordered computation itself (see Seamless Zen about add/multiply)
But another big part of this is the environment. We need an nvidia-cuda Docker image,
 will this make things reproducible regardless of host hardware/driver?)


Old plan
========

1. Rip GPU cell scheme, this will never work.
 Instead (long term plan):
 - Annotate transformers as GPU-based. This will make all of their pins GPU-based.
 - GPU-based transformers produce a GPU-based result. Checksum computation of this result is done on the GPU.
 - Maintain a checksum-to-GPUbuffer cache that maintains which buffers are on the GPU
   Can be done smarter for certain expressions (introduce offsets and stride)
   Cache can be emptied based on memory requirements, or if certain checksums are foreseen to be not needed
    anymore on the GPU.

New plan
========
- Calculate checksum on the CPU. Copying to the GPU will be a caching operation.
- Don't use unified memory access, since it isn't compatible with malloc (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-system-allocator).
- Tricky part is always the execution context, since CUDA contexts cannot be fork()'ed (!!!)
- CUDA low-level API is nice and simple. cuda2py gives a good example, albeit also of how not to do things.
  In short:     
    1. compile CUDA code to PTX data using nvcc (see cuda2py._py.Module)
    2. Import low-level API as lib, e.g. using pycuda.cuda, or directly using cffi (cuda2py._cffi).
       Start a CUDA context.
    3. Use cuModuleLoadData with PTX data
    4. Load function from module (cuda2py._py.Function)
    5a. Allocate buffers on the GPU
    5b. copy data (async?, stream?) to the GPU
    5c. build params array of GPU pointers (cuda2py does NOT do this correctly for npy arrays)
    6. Execute function using params array (cuLaunchKernel, stream?)
    7. Copy back result data. As this is in the order of gigabytes/sec, it won't be a big thing.
- From the 7 steps above, only do step 1 inside the transformation. A modified version of gen_header
  generates a kernel declaration of the form "extern C __global__ transform(...)". This declaration
  is added to the supplied CUDA code that is compiled to PTX.

  Delegate step 2-7 to a CUDA server that runs in a separate process 
  (network communication latency is microsecond range).
  The CUDA server receives and sends back only checksums, i.e. it requires a shared Redis (or delegate checksum 
  requests to Seamless instances connected via communion).
  Request format is very similar to transformation requests, however, the code buffer refers to PTX code, not Python.
  Input buffers can be kept GPU-allocated between requests, and liberated after some seconds or minutes.
  Each Seamless instance can use only a single CUDA server; write a smarter server if you want to use multi-GPU
  Some authentication might also be in order?
NOTE: The above is all for pure-CUDA transformers, i.e. wrapping a single kernel. This is in the spirit of pycuda,
and should be adaptable for OpenCL as well.
Alternatively: It should be already possible to mix in CUDA code objects into a regular compiled transformer 
This is because CUDA compiles to a standard .o that can be linked as usual (for functions declared as __global__).
In this case, the transformer code is responsible for creating a CUDA context and invoking kernels. The CUDA
context will not persist in between invocations of the transformer.
This is for transformations that use multiple kernels, or that use the same kernel multiple times.
This is closer to the normal usage of CUDA (as a C++ extension). 
(Finally, there is always the option of a Python transformer that calls pycuda all by itself). 
