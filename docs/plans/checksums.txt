Checksums
==========

The text below is a mixture of:
- Theoretical thoughts on checksums (should be delegated to theory at some point)
- Practical thoughts about a RPBS servers, including a generalized graph server

hash/checksum are SHA3-512:
  This uniquely defines what a cell or computation *is* (like Magnet URIs).
  Checksums of JSON structures containing checksums can be computed (aka Merkle trees)

global ID: an 128-bit unsigned integer that signifies a session ID
  (for interactive service instances) or a job ID (for non-interactive ones).
  The ID is guaranteed to be globally unique.
  The RPBS will maintain an ID server that returns the last ID + 1.
UPDATE: IMPORTANT INSIGHT: Replace "session" with "graph"!!!
As the RPBS, what you want to store, is a mapping of session to graph+dict,
 where the dict contains checksums that overrule those in the graph.
Session ID should be published to anyone who wishes to collaborate 
(so they can update the dict)
In contrast, re-binding an existing session to an updated graph is an
 RPBS admin operation (to fix a bug in a service).

In general, a computation is defined by the following:
(UPDATE: no more mid-level, nor reactor/macro. 
Send either a high-level context, or a low-level transformer. 
Same principles apply, though)
1. The type (context, transformer, reactor or macro)
   and the seamless version (more precisely, the version of the mid-level syntax)
2. A topology (mid-level declaration of contexts, cells, workers and connections)
   This includes the cell types, and the language of code cells, but no cell values
   NOTE: this is mid-level; the high-level must mark a context with its topology before
    it can be considered to be sent to a service.
   If type is "context": when marking, the topology cells are classified as:
   a. non-authoritative
   b. authoritative, but not connected to the outside
   c. authoritative, and connected to the outside.
   The server may impose restrictions, e.g. say that only category c. cells may be defined by the client,
    or refuse because a particular cell cannot be in category c. (for example, cells it considers private).
3. The value of authoritative cells, including the schemas.
  Checksums of those values will always be computed on the raw/text data (see above)
  The "grand cell value" includes the cell type (part of the topology).
4. Broader topology, that includes cells and workers that only define Report cells
   and Logging cells
   (UPDATE: do away with Report and Logging cells, see exception-logging.txt. They may only be connected to (normal) cells
   in *other* contexts)
5. Broader values:
  - Values of non-authoritative (i.e. result) cells
  - Values of transformer equilibrium state (UPDATE: ???)
  - Values of evaluation parameters and resource claims (UPDATE: i.e. meta-parameters that don't influence the computation result)
6. The environment. This includes:
  - Dependencies of the code (see dependency server above)
  - Docker version? Linux drivers?

I. Seamless "universal computation" dogma: a computation is defined by 1-3 only. A "grand checksum"
 of a computation is a single hash that uniquely defines 1-3.
4. and 5. are derivative data, they may be submitted, or added by the server.
II. Seamless "durable environment" dogma (see Seamless Zen)
When it comes to environments, every computation has only valid and invalid environments.
Valid environments give the (unique) correct result, whereas incorrect environments result in an error.
This means that a dependency library must never have the same code result in two different
 non-error results. The results must either be the same between two versions, or
 one version must give an error. If the library does not guarantee this, the code must do version checking.
NOTE: there is a "base environment" that escapes this dogma. The "base environment" includes
- Docker version
- Interpreter versions (Python, node.js)
- Compiler versions (GCC, CUDA)
- Relevant drivers and hardware (GPU, CPU instruction set), insofar exposed to the docker image
Memory does conform to the dogma, though, as does time-constrained execution.

When submitting, 1-3 is to check if the computation has been done. If not, 1-6 are submitted.
Submitting 4-6 (in full or in part) is optional: a server may infer it automatically.
A server may also refuse service because of values in 4-6.

Checksum/ID/cell servers
All of these servers are hosted at the RPBS, but they may forget entries after some time.
Seamless can be configured with a list of these servers that can be queried.

Computation hash-server:
hash1 => hash2. hash1 is the grand checksum of a computation, hash2 is its result.

Reverse computation hash-server:
Same as above, but hash2 => hash1. Note that the value underlying hash2 must be rather long for this to work,
 as there are several computations that can give the same result.

Computation server:
hash => computation. hash1 is the grand checksum of a computation JSON,
  computation is the computation JSON itself.
  Only non-error computations (status OK) are cached thus.

NOTE: make computation servers also for low-level transformers, and for reactors
 marked as "pure".
UPDATE: for now, this is *only* for low-level transformers. 
Need a separate server for high-level transformers, as well as for expressions.
These server DBs will *only* be filled by loading graphs in trusted mode
(see graph-management-todo.txt); a Seamless instance running at the RPBS
will do this continuously.
/UPDATE

Cell server:
hash+hash2 => value. Serves grand cell values (can be rather big). hash is the
hash of the cell value, whereas hash2 is the hash of the cell type (part of the topology).

Location server:
hash => (URI, mode). The hash is from a cell value.
Mode can be None.
If mode is "substitute", URI is a template into which the hash must be substituted, e.g. if URI is a cell server.

Description server:
hash => description. To give a semantic text description for a hash/

Context ID server:
ID => JSON, for context IDs. Returns an URI to a live interactive service instance.
  JSON has the format of   <protocol>: (URI, flag)
  protocol: dynamic HTML (websocket), two-way synchronization (websocket), one-way synchronization (REST)
  This URI must be unique for the ID. If the URI is dead, the instance will have died.
  Mode can be None.
  If mode is "substitute", URI is a template into which the hash must be substituted, e.g. if URI is a cell server.
  If mode is "submit", then the ID itself must be GETted/posted to the URI (together with any other data)
  RPBS server will accept only one registration per ID

Job ID server:
ID => JSON, for job IDs. Returns an URI to a job result of an atomic service.
  RPBS server will accept only one registration per ID.
  JSON has the format of   <protocol>: (URI, mode)
  protocol: one-way synchronization (REST), direct web service (REST), CGI server
  If mode is "substitute", URI is a template into which the hash must be substituted, e.g. if URI is a cell server.
  If mode is "submit", then the ID itself must be GETted/posted to the URI (together with any other data)
  RPBS server will accept only one registration per ID

Obsoletion server:
hash1+hash2 => hash3. Indicates that hash1 is now obsolete and should be replaced by hash3.
Reasons for obsoletion are: a new version, a bugfix, etc.
hash2 indicates the cell type.
hash1 must be rather long/specific for this to work.

Equivalence server:
hash1+hash2 => hash3. Same as obsoletion server, but indicates that hash3+hash2
is semantically equivalent to hash1+hash2.
This is typically because of adding comments, whitespace or reordering to a
source code, text or CSON file (normally not JSON, though)

Anathema server:
hash1 => hash2. hash1 is the grand checksum of a computation, hash2 is that of its result.
hash2 is anathema if it is fundamentally wrong, due to:
- Bugs in seamless
- Bugs in the library environment
- Bugs in Docker / kernel drivers
- Violation in the code of Seamless purity (cells with such code can also be stored in an "impurity server" as hash+hash2)
- Violation of the Seamless dependency dogma (e.g. logging cells connected to non-logging output cells)
  (such computations can also be stored in a "dogma violation server")
A Seamless server may consult the anathema server before returning its result.
If the underlying value of hash2 is sufficiently long, it may be stored in a separate "bogus server".

Finally: "marking".
Obsoletion server, reverse computation server, equivalence server, impurity server and bogus server require hashes of high-complexity values as an input. Otherwise, they are not an unique result.
Therefore, it is possible to make them accept one extra optional argument.
When registering a hash, a *random* 512-bit mark may be generated, and this may be used to mark even low-complexity
 cells (e.g. an integer cell with value "4", or an empty text cell).
Seamless will store the mark in the high-level context, and submit it to the service every time.

"Durability"
Environment dependencies may have different levels of durability (the Seamless durability dogma).
0. Zero durability. Results will be different, even with the same arguments.
   Example: Gromacs/AMBER and anything else that runs on the GPU
1. Low (exact version) durability. Results will be the same only with the exact same version.
   Example: ATTRACT, HADDOCK.
2. High (minor version) durability.
   Code is durable between two versions X and Y, that have the same major version,
     but where Y is later than X.
   2a. Code that runs under X will either give the same result as under Y, or give an exception    
   2a+. Code that runs under X will give the same result under Y (forward compatibility guaranteed)
   2b. Code that runs under Y will either give the same result under X, or give an exception (backward compatibility guaranteed).
   compatibility 2 means 2a & 2b. compatibility 2+ means 2a+ and 2b+.
   Examples: Python? (2+?)
3. Complete (major version) durability. The same, but Y and X do not need to have the same major version
   (or there is only one major version)
   Examples: POSIX tools?

RPBS will host a durability server.
For non-durable (incomplete durability) environment dependencies:
  Seamless versions will include a list of default dependency versions (for 1. and 2.) or minimum versions (for 2a. and 3a.)
  The + is just convenient for servers.
A durability overestimation of a computation is a special kind of bogus, it should be marked as such.
A zero-durability computation may still be stored, because the result is "as good as any".
When we start scientific reproducibility test servers, zero-durability computations must be repeated, together
 with computations that require a random seed (just change the seed). And of course, all computations must be either
 local or from trusted service servers, not from the scientists themselves.

UPDATE: for durability 1+, make a "requirement repo" that can automatically convert a list
 of requirement (e.g. a list of Python modules) to a list of version-stamped packages (pip, nix, apt). RPBS will a) be a mirror and b) snapshot package versions every month. Example: capability "numpy" + Nov 27 2018 => pip package numpy-1.15.4 (the state of PyPI on Nov 01 2018, i.e. "RPBS-Linux 18.11")
 Different capabilities can be combined and made into a Docker image using docker2repo/binder.

OLDER PLANS

Data management in Seamless
===========================


NOTE: The Seamless graph offers a unified format to describe:
- protocols (web services, unfilled web forms): a graph with code cells, undefined input cells, and undefined output cells.
- computations (jobs, filled-in web forms): a graph with code cells, input cells, and undefined output cells.
- results: a graph with code cells, input cells and output cells, where every cell is defined.
However, Seamless graphs contain no values at all, only checksums.
This means that they are small (and we should store them essentially forever)
On the other hand, this also means that every checksum needs to be backed up by data storage 
(checksum-to-value caching), else a Seamless graph becomes useless.
Therefore, from a Seamless point-of-view, data management is nothing more or less than *persistence*:
 deciding how long to maintain a checksum-to-value cache entry (*). 


Data security through external storage
======================================
We cannot guarantee data security at the RPBS. But sensitive data/code can be stored elsewere, let's 
say, at a hospital, and the data will never be accessed by the RPBS. Thus, data security is achieved.
Because: the checksum of the sensitive data/code will be publicly available, and the RPBS can 
still construct jobs that use these checksums, even if the data/code is stored at the hospital. 
This requires that the job is sent to Seamless-running-at-the hospital to be executed there.
Likewise, the hospital may choose to only send the job result checksum back, not its value. 
In that case, the RPBS can still store the job result checksum in the result graph, 
and store the result graph at the RPBS, so reproducibility is maintained.
But the job result value can then only be obtained by people at the hospital, and whoever they choose
 to share their data with (hopefully, the journal where they publish their results).


(*) = checksum-to-value cache, provenance cache, etc. are implemented as a single Redis database, backed up
 by a big hard disk. This Redis database will run on the "console" machine at the RPBS, and be accessible
 over the network, so every Seamless instance running on our cluster has direct access. This will make 
 Lustre superfluous).

/OLDER PLANS