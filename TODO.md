UPDATE OF THE UPDATE
Great Refactor is underway (see seamless-toward-02.md).
Most of the text below the FIRST section is out of date.
Things to do (all low level) :
- Finish destroy (see 0.1), for the sake of macro re-evaluation
- Proper exception reporting within exec (at least within IPython!)
- Mount: monitor write-only mount cells as well (give a warning, and re-write)
- exporting (any cell or pin; eliminate ExportedPin)
- symlinks  (may be exported)
- Macro sealing / connection layers / connections to macro-generated contexts
  NOTE: (low-level) contexts generated by macros must be *sealed*:
  - No children may be added to the context after sealing
  - Connections may only be added between A and B, where
    A is an exported child of the context, and B is a child of the direct parent context
    These connections must be built either a) in the macro that creates the direct parent context,
    or b) in a connection layer of the direct parent context  
  - When a parent context is sealed, so will all child contexts
  This is essential for proper caching; once a parent context is being sealed, the children
   are evaluated for cache hits (value hits).
  - Connections to such contexts are by default added to the *static connection layer* of the *current context*. The current context is the inner context whose macro is being run,
    or else the top-level context. A static connection layer consists of connections that are pairs of path names.
  - Such contexts whose macros are not in equilibrium have a special getattr, so you can build connections to not-yet-existing children.
- Signatures/checksums and macro caching. NOTE: take care of the alternative checksums caused by "cosmetic updates"
  (NOTE: macro caching is a low-level thing, because at the high-level, it is just a change-of-value of the macro parameter, no topology change!)
  Macro caching can give a topology hit (if the macro code and macro args are the same), and the resulting context may give value hits
   (depend on the value of the connected cells)

Then:
   - cson cells; also structured_cell in plain mode must be able to accommodate
      a cson's checksum is the checksum of the JSON representation; if you don't want that, connect a text cell downstream of it
   - Reactors (think of IPython stuff in the namespace, not properly addressed in 0.1; copy from 0.2 transformers)
     (also think of \_pending_inputs: add a name back in, if sent as None (like transformers))
   - Tie up loose ends of transfer protocol (copy, ref etc.)
   - "Active" switch of managers, workers, connections; may also be exported, and may be activated in a connection layer.
      UPDATE: partially done (for managers), extend to fine-grained level (maintain by manager)
   - Dynamic connection layers: are tied to a context, and take as input direct children of the context (cells or child contexts); may also set active switches
   - PyImport cells and code injection
   - Status dict, also as a structured cell  (also policies like .accept_shell_append)
   - Dynamic connection layers: a special macro that has one or more contexts as input (among other inputs), which must be (grand)children
      Builds connections within/between those contexts.
      No other connections may be built. In a later version, also support the addition of new cells (although these will never be cached)

Then, slowly move to the mid-level data structure:
Mostly elide the middle level, dynamically generate at time of low-level generation/serialization.
The middle level is the input of a translation macro, whereas the low level is the output
Normally:
1. Being under macro control, the lower level could never be authoritative
2. Any changes to the mid-level would re-trigger the translation macro.
=> Introduce an exception: sovereignty
A low level cell may be sovereign if it has a 1:1 correspondence to a mid-level element.
Sovereign cells are authoritative, they may be changed, and changes to sovereign cells do not cause the translation macro to re-trigger.
When a translation macro is re-triggered for another reason (or when the mid-level is serialized), the mid-level element is dynamically read from
 the sovereign cell (no double representation)
Then:
- apply to slash-0 (see mount.py:filehash)
- design mid-level AST, including old resources
Finally, the high level:
- High-level syntax, manipulating the mid-level AST. Syntax can be changed interactively if Silk is used.
- serialization (take care of shells also).
- high-level macros. They contain high-level syntax.
  They have, as an extra input, (a copy of) the high-level translation policies that were in effect at the time of creation

/FIRST section, beyond here is mostly out of date

UPDATE:
The following roadmap is outdated by the new conception of Silk as a schema language,
and the new high-level API
The topics are either:
- Explicitly covered by Silk/the high level API (see the docs there)
- Easily implemented at the library level, thanks to the high level API.
- Mechanics-based, and therefore unchanged (e.g. thread-based/process-based,
  sync/async workers)
That leaves three things to take care of:
(1) Macros that take a context argument (but I think this is documented already?)
(2) Event streams
(3) Equilibrium contexts.

(2) Event streams receive event values, or a "undo" signal, which means that all previous
values are invalid. Event streams may send back a "send again" signal, which means
that they want again all values that were previously sent to them. (This is for example if a transformer adds 5 to an event stream; if 5 is changed to 6, either in a input cell or by a change in the source code, the transformer will send a "send again" signal upstream).
Reactors may choose to cache events so to avoid sending this signal when one of their other inputs changes.
Finally, there are the "initialize" and "disconnect" signals.
Event stream inputpins are in a state where they accept a new value, or they don't. Force-feeding is possible, in that case the values are buffered up by seamless itself.
Workers must always declare explicitly a pin as event stream.
Transformers don't need any change in their code. However, if one or more of their inputs is an event stream, so must be their output (vice versa is not required, but if all inputs are cells, a transformer that has an event stream as output must return a list, which is force-fed into the event stream after sending an "initialize" signal).
Reactors must push/pull new event stream values in an explicit API:
  - blocking waits for it
  - non-blocking essentially sets to event stream input pin to "accept input" (input) or force-feeds (output)
  - By default, input is blocking while output is non-blocking
Event streams can never be authoritative, they must depend on a worker.

(3) Transformers are guaranteed not to send anything (be it cell values or events) on their primary output until execution has finished (which means they are in equilibrium).
In addition, transformers are guaranteed not to accept any events while not in equilibrium.
This is obviously not so for reactors, and it is also not so for contexts that contain reactors (or multiple transformers that are not arranged linearly) connected to context outputs.
It is possible to declare contexts as "equilibrium contexts". In that case, they have the same guarantees as transformers have: sending cell updates or events to the outside world is delayed until equilibrium is reached, and so is the acceptance of new events. This allows contexts to perform atomic computations, reducing the number of glitches.
It is possible to declare some of the outputs (and event stream inputs) as "secondary", which means that they escape this guarantee (for example, for logging purposes).






Technically-oriented releases are marked with *

\*0.1
After release, make videos:
  Basic example, based on examples/basic.py, then examples/basic-macro.py
  Fireworks
  3D
  Docking
  Orca (don't show the code)

0.2

- macros
I am not quite happy with how macros are being used. The direct import method
(defined macro "spam" in "eggs.py", "from eggs import spam") is fine for the
core macros, but it hampers live programming on other macros (though it isn't
prevented completely ; see test-dynamic-macro.py for an API example),
since it prohibits the link between macro <=> cell <=> file.
To solve that, a function .load_macro("spam", "eggs.py") is needed, that creates
a macro with a .cell attribute, with .cell.resource.filepath
(and .resource.lib) set properly. The .cell can be link()'ed as usual.
In addition, the function .load_block_macro("ham", "ham.py") loads ham.py as a
code block, i.e. adding "@macro", a def, and indenting the code.
ham.py can thus be a main script, like the ones in tests and examples.
For all main scripts in tests and examples, the "ctx = " and ctx.tofile
must be made conditional on __name__ == "__main__"

- Overhaul dtypes.objects, in particular the Python code blocks
  Allow Python blocks to be parsed by the transformer without return.
  Transformer execution is still prohibited without return, unless the transformer has no outputpin.
- Give a .as_cell() method every worker and context. This cell will contain a text representation
  (essentially invoking X_to_json).
  In the Python register, allow transformer.as_cell() to be registered as a Python function,
  and context/reactor.as_cell() as a Python class
- Replace the use of killable threads with processes... gives a problem with Orca example (fixed now ?), docking example (?), see Github issue
- Replace ctx.CHILDREN, ctx.CELLS etc. with ctx.self.children, ctx.self.cells, etc.
- Get rid of seamless.qt
- Composite (JSON) cells
- Expand and document seamless shell language (slash)
- Logging + dtype/worker documentation.resource system (using composite cells)
- Error message logging system (using composite cells)
- Overhaul dtypes, docson/type registration API, integrate with logging/documentation system. "array" and "json" are no longer dtypes, but formats
- Update demos

\*0.3
- Multiple code cells in transformers/reactors
- Preliminary outputpins (in transformers [as secondary output] and in reactors)
- Preliminary inputpins (pins that accept preliminary values). Right now, all inputpins are preliminary!
- Address shell() memory leak: IPython references may hold onto large amounts of data
- Address GLstore memory leak: stores may not be freed (?)
- Binary (struct) cells, implemented as structured "array" cells with dtype/shape/ndim (with functionality similarly to composite cells)
- Active switches (connection level; workers don't see it, except that pin becomes undefined/changes value)
- Silk: managing variable-length arrays with allocators (subclass ndarray), C header registrar, fix Bool default value bug + bug in examples/silk/test.py
- Document Silk, make it an official (supported) part of seamless
- C interop
- Game of Life demo with Cython and C
- Update OpenGL demos

0.4
- Finalize context graph format and their names, update tofile/fromfile accordingly
- Finalize resource management
- Finalize basic API, also how to change macros (SEE ABOVE)
- Cleanup code layout
- Document tofile/fromfile, saving options and seamless file format
- Code documentation + dtype/worker documentation system
- Set up user library directory and robogit
- Update demos

\*0.5
- Thread reactors, process reactors
- Synchronous transformers (do we need this?)
- Process transformers (now that execution is in a process, do we need this??)
- Cell arrays, channels, GUI-widget cells
- GPU computing (OpenCL)
- Update Game of Life demo

0.6
- Hook API and GUI for cell creation
- Update demos

0.7
- ATC, fold/unfold switches, Silk GUI generation
- More demos (tetris?)

\*0.8
- Python debugging, code editor (WIP)

\*0.9
- Collaborative protocol / delegated computing

\*1.0
- Lazy evaluation, GPU-GPU triggering
