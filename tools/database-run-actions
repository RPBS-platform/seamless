#!/usr/bin/env python

import os
import json
import cson
import csv
from ruamel.yaml import YAML
yaml = YAML(typ='safe')
import time
import re
import shutil
import subprocess
import multiprocessing
import concurrent.futures

from seamless import get_hash
from seamless.core.protocol.serialize import serialize_sync as serialize
from seamless.core.protocol.deserialize import deserialize_sync as deserialize

def err(*args, **kwargs):
    print("ERROR: " + args[0], *args[1:], **kwargs)
    exit(1)

def report(*args_, **kwargs):
    if not args.verbose:
        return
    print(*args_, **kwargs)

def write_csv(d:dict, filename):
    with open(filename, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile, delimiter=' ',
                                quotechar='|', quoting=csv.QUOTE_MINIMAL)
        for key, value in d.items():
            if isinstance(value, list):
                csvwriter.writerow([key] + value)
            elif isinstance(value, tuple):
                csvwriter.writerow([key] + list(value))
            else:
                csvwriter.writerow([key, value])

def load_csv(filename):
    result = {}
    with open(filename, newline='') as csvfile:
        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')
        for row in spamreader:
            if len(row) == 2:
                result[row[0]] = row[1]
            else:
                result[row[0]] = row[1:]
    return result

def get_plain_buffer(d:dict):
    """
    txt = json.dumps(d, sort_keys=True, indent=2)
    txt = txt.strip("\n") + "\n"
    return txt.encode()
    """
    return serialize(d, "plain")

def write_json(d:dict, filename):
    with open(filename, "wb") as f:
        f.write(get_plain_buffer(d))

def remove_stale_buffer_hardlink(checksum, inode):
    buffer_file = os.path.join(buffer_dir, checksum)
    if os.path.exists(buffer_file):
        buffer_inode = str(os.stat(buffer_file).st_ino)
        if buffer_inode == inode:
            report("Remove stale buffer hardlink {}".format(buffer_file))
            os.remove(buffer_file)

def intern_buffer(checksum, filename, hardlink):
    buffer_file = os.path.join(buffer_dir, checksum)
    if os.path.exists(buffer_file):
        # We trust that the buffer is correct...
        return
    if hardlink:
        os.link(filename, buffer_file)
    else:
        shutil.copyfile(filename, buffer_file)

def do_transform(cmd, source_celltype, target_celltype, tfs):
    # TODO: 
    # For conversion, try to use buffer_info entries in $SDB, and write them also!
    #
    # For cmd, try to synthesize a TransformationJob:
    #  e.g. bash transformer "ln -s inp inp.gz && gunzip -c inp.gz"
    # re-use existing TransformationJob results from jobs/, and write them also!
    result = []
    for checksum, filename in tfs:
        if cmd is not None:
            proc = subprocess.run(cmd + [filename], stdout=subprocess.PIPE, text=False)
            if proc.returncode != 0:
                if proc.returncode == -2:
                    raise KeyboardInterrupt
                print("Error in transforming {}".format(filename))            
            buf = proc.stdout
        else:
            with open(filename, "rb") as f:
                buf = f.read()
        result_checksum = get_hash(buf, hex=True)
        if source_celltype != target_celltype:
            """
            # too memory-intensive...
            cs = bytes.fromhex(result_checksum)
            conversion_result = try_convert(cs, source_celltype, target_celltype, buffer=buf)
            if conversion_result == True:
                result_checksum2 = result_checksum
            elif isinstance(conversion_result, bytes):
                result_checksum2 = conversion_result.hex()
            elif isinstance(conversion_result, str):
                result_checksum2 = bytes.fromhex(conversion_result).hex()
            else:
                raise Exception
            if result_checksum2 != result_checksum:
                result_checksum = result_checksum2
                buf = buffer_cache.get_buffer(result_checksum2)
            """
            value = deserialize(buf, bytes.fromhex(result_checksum), source_celltype, copy=False)
            if source_celltype == "bytes":
                try:
                    value = value.decode()
                except Exception:
                    pass
            newbuf = serialize(value, target_celltype)
            result_checksum2 = get_hash(newbuf, hex=True)
            buf = newbuf
            result_checksum = result_checksum2
        buffer_file = os.path.join(buffer_dir, result_checksum)
        if os.path.exists(buffer_file):
            # We trust that the buffer is correct...
            pass
        else:
            try:
                with open(buffer_file, "wb") as f:
                    f.write(buf)
            except Exception:
                os.remove(buffer_file)
        result.append((checksum, result_checksum))
    return result
    

SDB = os.environ.get("SEAMLESS_DATABASE_DIR")
if SDB is None:
    err("SEAMLESS_DATABASE_DIR undefined")

def get_subdir(subdirname):
    subdir = os.path.join(SDB, subdirname)
    if not os.path.exists(subdir):
        os.mkdir(subdir)
        report("Created {}".format(subdir))
    if not os.path.isdir(subdir):
        err("{} is not a directory".format(subdir))
    return subdir

import argparse
parser = argparse.ArgumentParser()
parser.add_argument(
    "actionfile",
    help="Database action file (json, cson or yaml) to run",
    type=argparse.FileType('r')
)
parser.add_argument(
    "--verbose", "-v",
    action="store_true"
)
parser.add_argument(
    "--force", "-f",
    action="store_true"
)

args = parser.parse_args()
afname = args.actionfile.name
if afname.endswith(".json"):
    loader = json.load
elif afname.endswith(".cson"):
    loader = cson.load
elif afname.endswith(".yaml"):
    loader = yaml.load
else:
    err("actionfile must be .json, .cson or .yaml")

try:
    actiondict = loader(args.actionfile)
except Exception as exc:
    err("could not parse actionfile")

report("Loaded actionfile:")
report(json.dumps(actiondict, indent=2))


# TODO: probably run actions against a JSON schema...
ok = False
if isinstance(actiondict, dict):
    while 1:
        if "directory" not in actiondict:
            break
        if "actions" not in actiondict:
            break
        ok = True
        break
if not ok:
    err("malformatted actionfile")
# /TODO

default_collection_name = actiondict["directory"]["collection"]
cross_device = actiondict["directory"]["cross_device"]
dirpath = actiondict["directory"]["path"]
if not os.path.exists(dirpath):
    err("{} does not exist".format(dirpath))
if not os.path.isdir(dirpath):
    err("{} is not a directory".format(dirpath))

# Build inode table. 
# If there is an old inode table:
#   remove interned buffers that point to an inode that has changed
inode_dir = get_subdir("inodes")
buffer_dir = get_subdir("buffers")
inode_table_file = os.path.join(inode_dir, default_collection_name) + ".csv"
try:
    old_inode_table = load_csv(inode_table_file)
except Exception as exc:
    old_inode_table = {}

inode_table = {}
inode_to_entry = {}
nfiles = 0
inodes_changed = False
last_inode_write = time.time()
for root, dirnames, filenames in os.walk(dirpath, followlinks=False):
    nfiles += len(filenames)
for root, dirnames, filenames in sorted(os.walk(dirpath, followlinks=False)):
    for filename in filenames:
        filepath = os.path.join(root, filename)
        if not root.startswith(dirpath): # would be weird...
            report("Error in {}".format(filepath))
            continue
        entry = os.path.join(root[len(dirpath):], filename).lstrip("/")
        try:
            inode = str(os.stat(filepath, follow_symlinks=False).st_ino)
            mtime = str(os.stat(filepath, follow_symlinks=False).st_mtime)
        except OSError:
            report("Error in {}".format(filepath))
            continue
        
        inode_to_entry[inode] = entry
        from_cache = False
        if inode in old_inode_table:
            v = old_inode_table[inode]
            if isinstance(v, list) and len(v) == 2:
                old_checksum, old_mtime = v
                if mtime == old_mtime:
                    inode_table[inode] = v
                    from_cache = True

        if not from_cache:            
            try:
                content = open(filepath, "rb").read()
            except OSError:
                report("Error in {}".format(filepath))
                continue
            checksum = get_hash(content, hex=True)
            if inode in old_inode_table:
                # If there is a buffer hardlink with the old checksum 
                #  that points to the inode, clean it up
                old_checksum, _ = old_inode_table[inode]
                remove_stale_buffer_hardlink(old_checksum, inode)
            inode_table[inode] = [checksum, mtime]
            inodes_changed = True

            if len(inode_table) % 1000 == 0:
                report("Calculated checksums for {}/{} files".format(
                    len(inode_table), nfiles
                ))
        elapsed_time = time.time()
        if elapsed_time - last_inode_write > 20:
            report("Save intermediate inode table to {}".format(inode_table_file)) 
            curr_inode_table = old_inode_table.copy()
            curr_inode_table.update(inode_table)
            write_csv(inode_table, inode_table_file)
            last_inode_write = elapsed_time

if not inodes_changed:
    if args.force:
        report("Inode table unchanged, but execution is forced")
    else:
        print("""Inode table unchanged. 
Unless the action file was modified, there is nothing to do. 
Use --force to force execution""")
        exit(0)
else:
    report("Write inode table to {}".format(inode_table_file))
    write_csv(inode_table, inode_table_file)

report("Running actions")
collections = {
    default_collection_name: {
        "name": default_collection_name,
        "default": True
    }
}
actions = actiondict.get("actions", [])


with concurrent.futures.ProcessPoolExecutor() as executor:
    for action in actions:
        name = action["action"]
        
        title = action.get("collection")
        if title is None:
            title = action.get("source_collection", default_collection_name)
        elif "source_collection" in action:
            title = action["source_collection"] + " => " + title
        report('Action {} "{}"'.format(name, title))
        if name == "intern_collection":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]        
            hardlink = action["hardlink"]
            buffer_dir = get_subdir("buffers")
            if source_collection.get("default"):
                if hardlink and cross_device:
                    # TODO: do such checks before running any actions
                    err("Cross-device directory cannot be interned with hardlinks")
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    filename = os.path.join(dirpath, entry)
                    intern_buffer(checksum, filename, hardlink)
                    
            elif source_collection.get("copied"):
                raise NotImplementedError # TODO: make a test for this
            else:
                msg = 'Action "intern_collection": source collection "{}" must be default or copied'
                err(msg.format(source_collection_name))
            
            source_collection["interned"] = True

        elif name == "transform_collection":
            nproc = max(int(multiprocessing.cpu_count()/2), 1)
            target_collection_name = action["collection"]
            command = action.get("command")
            assert command in ("gunzip", "unzip", "bunzip2", None)
            if command == "gunzip":
                cmd = ["gunzip", "-c"]
            elif command == "bunzip2":
                cmd = ["bunzip2", "-c"]
            elif command == "unzip":
                cmd = ["unzip", "-p"]
            else:
                cmd = None
            target_celltype = action.get("celltype")
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]        
            buffer_dir = get_subdir("buffers")
            transform_dir = get_subdir("transforms")
            transform_file = os.path.join(transform_dir, target_collection_name) + ".csv"
            if os.path.exists(transform_file):
                old_transform = load_csv(transform_file)
            else:
                old_transform = {}

            transform = {}
            tfs = []
            ntransform = 0
            if source_collection.get("default"):
                origin_collection_name = default_collection_name
                origin_collection = collections[default_collection_name]
                source_celltype = "bytes"
                done_checksums = set()
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    if checksum in old_transform:
                        result_checksum = old_transform[checksum]
                        result_checksum_file = os.path.join(buffer_dir, result_checksum)
                        if os.path.exists(result_checksum_file):
                            ntransform += 1
                            transform[checksum] = result_checksum
                            continue                
                    if checksum in done_checksums:
                        continue
                    ntransform += 1                
                    done_checksums.add(checksum)
                    entry = inode_to_entry[inode]
                    tf = os.path.join(dirpath, entry)
                    tfs.append((checksum, tf))
            elif source_collection.get("transformed"):
                raise NotImplementedError # TODO: make a test for this
                origin_collection_name = source_collection["origin"]
                origin_collection = collections[origin_collection_name]
                source_celltype = source_collection.get("celltype", "bytes")
            elif source_collection.get("copied"):
                origin_collection = source_collection
                origin_collection_name = source_collection["name"]
                source_celltype = source_collection.get("celltype", "bytes")
                collection_dir = get_subdir("collections")
                source_collection_dir = os.path.join(collection_dir, source_collection_name)
                result_file = os.path.join(collection_dir, source_collection_name) + ".csv"
                result = load_csv(result_file)
                done_checksums = set()
                for entry, checksum in result.items():
                    if checksum in old_transform:
                        result_checksum = old_transform[checksum]
                        result_checksum_file = os.path.join(buffer_dir, result_checksum)
                        if os.path.exists(result_checksum_file):
                            ntransform += 1
                            transform[checksum] = result_checksum
                            continue                
                    if checksum in done_checksums:
                        continue
                    ntransform += 1                
                    done_checksums.add(checksum)
                    tf = os.path.join(source_collection_dir, entry)
                    tfs.append((checksum, tf))
            else:
                raise AssertionError(source_collection)

            futures = []
            chunksize = 100
            last_transform_write = time.time()
            curr_tfs = []
            assert source_celltype is not None
            if target_celltype is None:
                target_celltype = source_celltype
            for n in range(len(tfs)):
                tf = tfs[n]
                curr_tfs.append(tf)
                last = (n == len(tfs) - 1)
                if len(curr_tfs) == chunksize or last: 
                    future = executor.submit(
                        do_transform, cmd, 
                        source_celltype, target_celltype,
                        curr_tfs
                    )
                    futures.append(future)
                    curr_tfs = []
                while len(futures) and (len(futures) == nproc or last):
                    for future in futures:
                        if not future.done():
                            continue
                        result = future.result()
                        futures.remove(future)
                        for checksum, result_checksum in result:
                            if result_checksum is not None:
                                transform[checksum] = result_checksum
                                if len(transform) % 1000 == 0:
                                    report("Transformed {}/{} files".format(
                                        len(transform), ntransform
                                    ))

                                elapsed_time = time.time()
                                if elapsed_time - last_transform_write > 20:
                                    report("Save intermediate transform table to {}".format(transform_file)) 
                                    write_csv(transform, transform_file)
                                    last_transform_write = elapsed_time
                        break
                    else:
                        time.sleep(1)
            assert not len(futures)
            write_csv(transform, transform_file)
            collections[target_collection_name] = {
                "name": target_collection_name,
                "transformed": True,
                "celltype": target_celltype,
                "origin": origin_collection_name
            }
                        
        elif name == "copy_collection":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]                    
            target_collection_name = action["collection"]
            collection_dir = get_subdir("collections")
            target_collection_dir = os.path.join(collection_dir, target_collection_name)
            buffer_dir = get_subdir("buffers")
            interned = False
            can_hardlink = False
            if source_collection.get("default"):
                celltype = None
                origin_collection_name = default_collection_name
                origin_collection = collections[origin_collection_name]
                can_hardlink = cross_device
                if origin_collection.get("interned"):
                    interned = True
            elif source_collection.get("transformed"):
                origin_collection_name = source_collection["origin"]
                origin_collection = collections[origin_collection_name]
                can_hardlink = True
                interned = True
                celltype = source_collection["celltype"]
                if celltype == "bytes":
                    celltype = None
            elif source_collection.get("copied"):
                can_hardlink = True
                interned = True
                celltype = source_collection.get("celltype")
                if celltype == "bytes":
                    celltype = None
            else:
                raise AssertionError(source_collection)
            hardlink = action["hardlink"]
            if hardlink and not can_hardlink:
                msg = 'Action "copy_collection": source collection "{}" cannot be hardlinked'
                err(msg.format(source_collection_name))

            raw_filenames = {}
            origin_checksums = {}
            if origin_collection.get("default"):
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    origin_checksums[entry] = checksum
                    raw_filenames[entry] = os.path.join(dirpath, entry)
            elif origin_collection.get("copied"):
                collection_dir = get_subdir("collections")
                origin_collection_dir = os.path.join(collection_dir, origin_collection["name"])
                result_file = os.path.join(collection_dir, origin_collection_name) + ".csv"
                result = load_csv(result_file)
                deepcell = {}
                for entry, checksum in result.items():
                    origin_checksums[entry] = checksum
                    raw_filenames[entry] = os.path.join(origin_collection_dir, entry)
            else:
                raise AssertionError(origin_collection)

            if source_collection.get("transformed"):
                transform_dir = get_subdir("transforms")
                transform_file = os.path.join(transform_dir, source_collection_name) + ".csv"
                transform = load_csv(transform_file)
                checksums = {entry: transform[checksum] for entry, checksum in origin_checksums.items()}
            else:
                checksums = origin_checksums

            if interned:
                filenames = {
                    entry:os.path.join(buffer_dir, checksum)
                    for entry, checksum in checksums.items()
                }
            else:
                filenames = raw_filenames

            result = {}
            regex = re.compile(action["source_file"])
            target_file_template = action["target_file"]
            to_copy = []
            count = 0

            for source_entry, checksum in checksums.items():
                if count > 0 and count % 50000 == 0:
                    report("Checking {}/{} files".format(
                        count, len(checksums)
                    ))      
                count += 1             
                filename = filenames[source_entry]
                unnamed_capturing_groups = regex.match(source_entry).groups()
                named_capturing_groups = regex.match(source_entry).groupdict()                
                target_entry = target_file_template.format(
                    *unnamed_capturing_groups,
                    **named_capturing_groups
                )
                result[target_entry] = checksum

                target_file = os.path.join(target_collection_dir, target_entry)
                if os.path.exists(target_file):
                    old_ino = os.stat(target_file).st_ino
                    if hardlink:
                        ino = os.stat(filename).st_ino
                        if old_ino == ino:
                            continue
                    buf_file = os.path.join(buffer_dir, checksum)
                    if os.path.exists(buf_file):
                        ino = os.stat(buf_file).st_ino
                        if old_ino == ino:
                            continue
                    os.remove(target_file)
                to_copy.append((filename, target_file))
            for n in range(len(to_copy)):
                filename, target_file = to_copy[n]
                if n > 0 and n % 20000 == 0:
                    term = "Hardlink" if hardlink else "Copy"
                    report("{} {}/{} files".format(
                        term, n, len(to_copy)
                    ))                    
                os.makedirs(os.path.dirname(target_file), exist_ok=True)
                if hardlink:
                    os.link(filename, target_file)
                else:
                    shutil.copyfile(filename, target_file)

            result_file = os.path.join(collection_dir, target_collection_name) + ".csv"
            old_result = {}
            if os.path.exists(result_file):
                old_result = load_csv(result_file)
            for target_entry in old_result:
                if target_entry not in result:
                    target_file = os.path.join(target_collection_dir, target_entry)
                    if os.path.exists(target_file):
                        os.remove(target_file)
            write_csv(result, result_file)
            collections[target_collection_name] = {
                "name": target_collection_name,
                "copied": True,
            }
            if celltype is not None:
                collections[target_collection_name]["celltype"] = celltype
        
        elif name == "build_download_index":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]
            if source_collection.get("default"):
                entries = inode_to_entry.values()
            elif source_collection.get("copied"):
                collection_dir = get_subdir("collections")
                result_file = os.path.join(collection_dir, source_collection_name) + ".csv"
                result = load_csv(result_file)
                entries = result.keys()
            else:
                msg = 'Action "build_download_index": source collection "{}" must be default or copied'
                err(msg.format(source_collection_name))
            
            download_index = {}
            urls = action["urls"]
            regex = re.compile(action["source_file"])
            for entry in entries:
                unnamed_capturing_groups = regex.match(entry).groups()
                named_capturing_groups = regex.match(entry).groupdict()
                target_urls = []
                for urldict in urls:
                    if isinstance(urldict, str):
                        urldict = {"url": urldict}
                    target = {}
                    for attr in "compression", "celltype":
                        if attr in urldict:
                            target[attr] = urldict[attr]
                    target["url"] = urldict["url"].format(
                        *unnamed_capturing_groups,
                        **named_capturing_groups
                    )
                    target_urls.append(target)
                download_index[entry] = target_urls
            download_index_dir = get_subdir("download_indices")
            download_index_file = os.path.join(download_index_dir, source_collection_name) + ".json"
            report("Write download index to {}".format(download_index_file))
            write_json(download_index, download_index_file)
        
        elif name == "dataset":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]
            dataset_dir = get_subdir("datasets")
            dataset_file = os.path.join(dataset_dir, source_collection_name) + ".json"
            if os.path.exists(dataset_file):
                old_dataset = json.load(open(dataset_file))
                old_dataset_checksum = get_hash(get_plain_buffer(old_dataset), hex=True)
                old_dataset_inode = str(os.stat(dataset_file).st_ino)
            else:
                old_dataset_checksum = None
            dataset = {}
            if source_collection.get("default"):
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    dataset[entry] = checksum        
            elif source_collection.get("copied"):
                collection_dir = get_subdir("collections")
                result_file = os.path.join(collection_dir, target_collection_name) + ".csv"
                result = load_csv(result_file)
                dataset = result
            else:
                msg = 'Action "dataset": source collection "{}" must be default or copied'
                err(msg.format(source_collection_name))

            dataset_checksum = get_hash(get_plain_buffer(dataset), hex=True)
            print("Computed dataset for {}: deep buffer checksum {}".format(source_collection_name, dataset_checksum))
            if old_dataset_checksum is not None and dataset_checksum != old_dataset_checksum:
                remove_stale_buffer_hardlink(old_dataset_checksum, old_dataset_inode)
            report("Write dataset deep buffer to {}".format(dataset_file))
            write_json(dataset, dataset_file)
            intern_buffer(dataset_checksum, dataset_file, hardlink=True)
        
        elif name == "deepcell":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]
            deepcell_dir = get_subdir("deepcells")
            deepcell_file = os.path.join(deepcell_dir, source_collection_name) + ".json"
            if os.path.exists(deepcell_file):
                old_deepcell = json.load(open(deepcell_file))
                old_deepcell_checksum = get_hash(get_plain_buffer(old_deepcell), hex=True)
                old_deepcell_inode = str(os.stat(deepcell_file).st_ino)
            else:
                old_deepcell_checksum = None
            deepcell = {}
            if source_collection.get("default"):
                celltype = None
            elif source_collection.get("copied"):
                celltype = source_collection.get("celltype")
            elif source_collection.get("transformed"):
                celltype = source_collection.get("celltype")
            else:
                celltype = None
            
            if celltype != "mixed":
                msg = 'Action "deepcell": source collection "{}" must have celltype "mixed"'
                err(msg.format(source_collection_name))

            regex = re.compile(action["source_file"])
            target_key_template = action["target_key"]

            if source_collection.get("copied"):
                raise NotImplementedError # TODO: a test for this
                collection_dir = get_subdir("collections")
                result_file = os.path.join(collection_dir, source_collection_name) + ".csv"
                result = load_csv(result_file)
                deepcell0 = result
            elif source_collection.get("transformed"):
                transform_dir = get_subdir("transforms")
                transform_file = os.path.join(transform_dir, source_collection_name) + ".csv"
                transform = load_csv(transform_file)
                origin_collection_name = source_collection["origin"]
                origin_collection = collections[origin_collection_name]

                if origin_collection.get("default"):
                    raise NotImplementedError # TODO: a test for this
                    origin_checksums = {}
                    deepcell0 = {}
                    for inode in inode_table:
                        checksum, _ = inode_table[inode]
                        entry = inode_to_entry[inode]
                        deepcell0[entry] = transform[checksum]
                elif origin_collection.get("copied"):
                    collection_dir = get_subdir("collections")
                    result_file = os.path.join(collection_dir, origin_collection_name) + ".csv"
                    result = load_csv(result_file)
                    deepcell0 = {}
                    for entry, checksum in result.items():
                        deepcell0[entry] = transform[checksum]
                else:
                    raise AssertionError(origin_collection)
            else:
                raise AssertionError(source_collection)

            deepcell = {}
            for entry, checksum in deepcell0.items():
                unnamed_capturing_groups = regex.match(entry).groups()
                named_capturing_groups = regex.match(entry).groupdict()                
                target_entry = target_key_template.format(
                    *unnamed_capturing_groups,
                    **named_capturing_groups
                )
                deepcell[target_entry] = checksum

            deepcell_checksum = get_hash(get_plain_buffer(deepcell), hex=True)
            print("Computed deepcell for {}: deep buffer checksum {}".format(source_collection_name, deepcell_checksum))
            if old_deepcell_checksum is not None and deepcell_checksum != old_deepcell_checksum:
                remove_stale_buffer_hardlink(old_deepcell_checksum, old_deepcell_inode)
            report("Write deepcell deep buffer to {}".format(deepcell_file))
            write_json(deepcell, deepcell_file)
            intern_buffer(deepcell_checksum, deepcell_file, hardlink=True)
