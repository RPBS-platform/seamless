#!/usr/bin/env python

import os
import json
import cson
import csv
from ruamel.yaml import YAML
yaml = YAML(typ='safe')
from hashlib import sha3_256
import time
import re
import shutil
import subprocess
import multiprocessing
import concurrent.futures

def get_hash(content, hex=False):
    if isinstance(content, str):
        content = content.encode()
    if not isinstance(content, bytes):
        raise TypeError(type(content))
    hash = sha3_256(content)
    result = hash.digest()
    if hex:
        result = result.hex()
    return result

def err(*args, **kwargs):
    print("ERROR: " + args[0], *args[1:], **kwargs)
    exit(1)

def report(*args_, **kwargs):
    if not args.verbose:
        return
    print(*args_, **kwargs)

def write_csv(d:dict, filename):
    with open(filename, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile, delimiter=' ',
                                quotechar='|', quoting=csv.QUOTE_MINIMAL)
        for key, value in d.items():
            if isinstance(value, list):
                csvwriter.writerow([key] + value)
            elif isinstance(value, tuple):
                csvwriter.writerow([key] + list(value))
            else:
                csvwriter.writerow([key, value])

def load_csv(filename):
    result = {}
    with open(filename, newline='') as csvfile:
        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')
        for row in spamreader:
            if len(row) == 2:
                result[row[0]] = row[1]
            else:
                result[row[0]] = row[1:]
    return result

def get_plain_buffer(d:dict):
    txt = json.dumps(d, sort_keys=True, indent=2)
    txt = txt.strip("\n") + "\n"
    return txt.encode()

def write_json(d:dict, filename):
    with open(filename, "wb") as f:
        f.write(get_plain_buffer(d))

def remove_stale_buffer_hardlink(checksum, inode):
    buffer_file = os.path.join(buffer_dir, checksum)
    if os.path.exists(buffer_file):
        buffer_inode = str(os.stat(buffer_file).st_ino)
        if buffer_inode == inode:
            report("Remove stale buffer hardlink {}".format(buffer_file))
            os.remove(buffer_file)

def intern_buffer(checksum, filename, hardlink):
    buffer_file = os.path.join(buffer_dir, checksum)
    if os.path.exists(buffer_file):
        # We trust that the buffer is correct...
        return
    if hardlink:
        os.link(filename, buffer_file)
    else:
        shutil.copyfile(filename, buffer_file)

def do_transform(cmd, tfs):
    result = []
    for checksum, filename in tfs:
        proc = subprocess.run(cmd + [filename], stdout=subprocess.PIPE, text=False)
        if proc.returncode != 0:
            if proc.returncode == -2:
                raise KeyboardInterrupt
            print("Error in transforming {}".format(filename))            
        buf = proc.stdout
        result_checksum = get_hash(buf, hex=True)
        buffer_file = os.path.join(buffer_dir, result_checksum)
        if os.path.exists(buffer_file):
            # We trust that the buffer is correct...
            pass
        else:
            try:
                with open(buffer_file, "wb") as f:
                    f.write(buf)
            except Exception:
                os.remove(buffer_file)
        result.append((checksum, result_checksum))
    return result
    

SDB = os.environ.get("SEAMLESS_DATABASE_DIR")
if SDB is None:
    err("SEAMLESS_DATABASE_DIR undefined")

def get_subdir(subdirname):
    subdir = os.path.join(SDB, subdirname)
    if not os.path.exists(subdir):
        os.mkdir(subdir)
        report("Created {}".format(subdir))
    if not os.path.isdir(subdir):
        err("{} is not a directory".format(subdir))
    return subdir

import argparse
parser = argparse.ArgumentParser()
parser.add_argument(
    "actionfile",
    help="Database action file (json, cson or yaml) to run",
    type=argparse.FileType('r')
)
parser.add_argument(
    "--verbose", "-v",
    action="store_true"
)
parser.add_argument(
    "--force", "-f",
    action="store_true"
)

args = parser.parse_args()
afname = args.actionfile.name
if afname.endswith(".json"):
    loader = json.load
elif afname.endswith(".cson"):
    loader = cson.load
elif afname.endswith(".yaml"):
    loader = yaml.load
else:
    err("actionfile must be .json, .cson or .yaml")

try:
    actiondict = loader(args.actionfile)
except Exception as exc:
    err("could not parse actionfile")

report("Loaded actionfile:")
report(json.dumps(actiondict, indent=2))


# TODO: probably run actions against a JSON schema...
ok = False
if isinstance(actiondict, dict):
    while 1:
        if "directory" not in actiondict:
            break
        if "actions" not in actiondict:
            break
        ok = True
        break
if not ok:
    err("malformatted actionfile")
# /TODO

default_collection_name = actiondict["directory"]["collection"]
cross_device = actiondict["directory"]["cross_device"]
dirpath = actiondict["directory"]["path"]
if not os.path.exists(dirpath):
    err("{} does not exist".format(dirpath))
if not os.path.isdir(dirpath):
    err("{} is not a directory".format(dirpath))

# Build inode table. 
# If there is an old inode table:
#   remove interned buffers that point to an inode that has changed
inode_dir = get_subdir("inodes")
buffer_dir = get_subdir("buffers")
inode_table_file = os.path.join(inode_dir, default_collection_name) + ".csv"
try:
    old_inode_table = load_csv(inode_table_file)
except Exception as exc:
    old_inode_table = {}

inode_table = {}
inode_to_entry = {}
nfiles = 0
inodes_changed = False
last_inode_write = time.time()
for root, dirnames, filenames in os.walk(dirpath, followlinks=False):
    nfiles += len(filenames)
for root, dirnames, filenames in sorted(os.walk(dirpath, followlinks=False)):
    for filename in filenames:
        filepath = os.path.join(root, filename)
        if not root.startswith(dirpath): # would be weird...
            report("Error in {}".format(filepath))
            continue
        entry = os.path.join(root[len(dirpath):], filename).lstrip("/")
        try:
            inode = str(os.stat(filepath, follow_symlinks=False).st_ino)
            mtime = str(os.stat(filepath, follow_symlinks=False).st_mtime)
        except OSError:
            report("Error in {}".format(filepath))
            continue
        
        inode_to_entry[inode] = entry
        from_cache = False
        if inode in old_inode_table:
            v = old_inode_table[inode]
            if isinstance(v, list) and len(v) == 2:
                old_checksum, old_mtime = v
                if mtime == old_mtime:
                    inode_table[inode] = v
                    from_cache = True

        if not from_cache:            
            try:
                content = open(filepath, "rb").read()
            except OSError:
                report("Error in {}".format(filepath))
                continue
            checksum = get_hash(content, hex=True)
            if inode in old_inode_table:
                # If there is a buffer hardlink with the old checksum 
                #  that points to the inode, clean it up
                old_checksum, _ = old_inode_table[inode]
                remove_stale_buffer_hardlink(old_checksum, inode)
            inode_table[inode] = [checksum, mtime]
            inodes_changed = True

            if len(inode_table) % 1000 == 0:
                report("Calculated checksums for {}/{} files".format(
                    len(inode_table), nfiles
                ))
        elapsed_time = time.time()
        if elapsed_time - last_inode_write > 20:
            report("Save intermediate inode table to {}".format(inode_table_file)) 
            curr_inode_table = old_inode_table.copy()
            curr_inode_table.update(inode_table)
            write_csv(inode_table, inode_table_file)
            last_inode_write = elapsed_time

if not inodes_changed:
    if args.force:
        report("Inode table unchanged, but execution is forced")
    else:
        print("""Inode table unchanged. 
Unless the action file was modified, there is nothing to do. 
Use --force to force execution""")
        exit(0)
else:
    report("Write inode table to {}".format(inode_table_file))
    write_csv(inode_table, inode_table_file)

report("Running actions")
collections = {
    default_collection_name: {
        "default": True
    }
}
actions = actiondict.get("actions", [])


with concurrent.futures.ProcessPoolExecutor() as executor:
    for action in actions:
        name = action["action"]
        
        report("Action {}".format(name))
        if name == "intern_collection":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]        
            hardlink = action["hardlink"]
            buffer_dir = get_subdir("buffers")
            if source_collection.get("default"):
                if hardlink and cross_device:
                    # TODO: do such checks before running any actions
                    err("Cross-device directory cannot be interned with hardlinks")
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    filename = os.path.join(dirpath, entry)
                    intern_buffer(checksum, filename, hardlink)
                    
            elif source_collection.get("copied"):
                raise NotImplementedError
            else:
                msg = 'Action "intern_collection": source collection "{}" must be default or copied'
                err(msg.format(source_collection_name))
            
            source_collection["interned"] = True

        elif name == "transform_collection":
            nproc = max(int(multiprocessing.cpu_count()/2), 1)
            target_collection_name = action["collection"]            
            if action.get("celltype") not in (None, "bytes"):
                raise NotImplementedError
            command = action.get("command")
            assert command in ("gunzip", "unzip", "bunzip2", None)
            if command != "gunzip":
                raise NotImplementedError
            cmd = ["gunzip", "-c"]
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]        
            buffer_dir = get_subdir("buffers")
            transform_dir = get_subdir("transforms")
            transform_file = os.path.join(transform_dir, target_collection_name) + ".csv"
            if os.path.exists(transform_file):
                old_transform = load_csv(transform_file)
            else:
                old_transform = {}

            transform = {}
            tfs = []
            ntransform = 0
            if source_collection.get("default"):
                origin_collection = default_collection_name
                done_checksums = set()
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    if checksum in old_transform:
                        result_checksum = old_transform[checksum]
                        result_checksum_file = os.path.join(buffer_dir, result_checksum)
                        if os.path.exists(result_checksum_file):
                            ntransform += 1
                            transform[checksum] = result_checksum
                            continue                
                    if checksum in done_checksums:
                        continue
                    ntransform += 1                
                    done_checksums.add(checksum)
                    entry = inode_to_entry[inode]
                    tf = os.path.join(dirpath, entry)
                    tfs.append((checksum, tf))
            elif source_collection.get("transformed"):
                raise NotImplementedError
                origin_collection = source_collection["origin"]
            elif source_collection.get("copied"):
                raise NotImplementedError
            else:
                raise AssertionError(source_collection)

            futures = []
            chunksize = 100
            last_transform_write = time.time()
            curr_tfs = []
            for n in range(len(tfs)):
                tf = tfs[n]
                curr_tfs.append(tf)
                last = (n == len(tfs) - 1)
                if len(curr_tfs) == chunksize or last: 
                    future = executor.submit(do_transform, cmd, curr_tfs)
                    futures.append(future)
                    curr_tfs = []
                while len(futures) and (len(futures) == nproc or last):
                    for future in futures:
                        if not future.done():
                            continue
                        result = future.result()
                        futures.remove(future)
                        for checksum, result_checksum in result:
                            if result_checksum is not None:
                                transform[checksum] = result_checksum
                                if len(transform) % 1000 == 0:
                                    report("Transformed {}/{} files".format(
                                        len(transform), ntransform
                                    ))

                                elapsed_time = time.time()
                                if elapsed_time - last_transform_write > 20:
                                    report("Save intermediate transform table to {}".format(transform_file)) 
                                    write_csv(transform, transform_file)
                                    last_transform_write = elapsed_time
                        break
                    else:
                        time.sleep(1)
            assert not len(futures)
            write_csv(transform, transform_file)
            collections[target_collection_name] = {
                "transformed": True,
                "origin": origin_collection
            }
                        
        elif name == "copy_collection":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]                    
            target_collection_name = action["collection"]
            collection_dir = get_subdir("collections")
            target_collection_dir = os.path.join(collection_dir, target_collection_name)
            buffer_dir = get_subdir("buffers")
            interned = False
            can_hardlink = False
            if source_collection.get("default"):
                origin_collection_name = default_collection_name
                can_hardlink = cross_device
                if origin_collection.get("interned"):
                    interned = True
            elif source_collection.get("transformed"):
                origin_collection_name = source_collection["origin"]
                can_hardlink = True
                interned = True
            elif source_collection.get("copied"):
                raise NotImplementedError # TODO: a test for this
                can_hardlink = True
                interned = True
            else:
                raise AssertionError(source_collection)
            hardlink = action["hardlink"]
            if hardlink and not can_hardlink:
                msg = 'Action "copy_collection": source collection "{}" cannot be hardlinked'
                err(msg.format(source_collection_name))

            origin_collection = collections[origin_collection_name]
            raw_filenames = {}
            if origin_collection.get("default"):
                origin_checksums = {}
                filenames = {}
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    origin_checksums[entry] = checksum
                    raw_filenames[entry] = os.path.join(dirpath, entry)
            elif origin_collection.get("copied"):
                raise NotImplementedError
            else:
                raise AssertionError(origin_collection)

            if source_collection.get("transformed"):
                transform_dir = get_subdir("transforms")
                transform_file = os.path.join(transform_dir, source_collection_name) + ".csv"
                transform = load_csv(transform_file)
                checksums = {entry: transform[checksum] for entry, checksum in origin_checksums.items()}
            else:
                checksums = origin_checksums

            if interned:
                filenames = {
                    entry:os.path.join(buffer_dir, checksum)
                    for entry, checksum in checksums.items()
                }
            else:
                filenames = raw_filenames

            result = {}
            regex = re.compile(action["source_file"])
            target_file_template = action["target_file"]
            to_copy = []
            count = 0

            for source_entry, checksum in checksums.items():
                if count > 0 and count % 50000 == 0:
                    report("Checking {}/{} files".format(
                        count, len(checksums)
                    ))      
                count += 1             
                filename = filenames[source_entry]
                unnamed_capturing_groups = regex.match(source_entry).groups()
                named_capturing_groups = regex.match(source_entry).groupdict()                
                target_entry = target_file_template.format(
                    *unnamed_capturing_groups,
                    **named_capturing_groups
                )
                result[target_entry] = checksum

                target_file = os.path.join(target_collection_dir, target_entry)
                if os.path.exists(target_file):
                    old_ino = os.stat(target_file).st_ino
                    if hardlink:
                        ino = os.stat(filename).st_ino
                        if old_ino == ino:
                            continue
                    buf_file = os.path.join(buffer_dir, checksum)
                    if os.path.exists(buf_file):
                        ino = os.stat(buf_file).st_ino
                        if old_ino == ino:
                            continue
                    os.remove(target_file)
                to_copy.append((filename, target_file))
            for n in range(len(to_copy)):
                filename, target_file = to_copy[n]
                if n > 0 and n % 20000 == 0:
                    term = "Hardlink" if hardlink else "Copy"
                    report("{} {}/{} files".format(
                        term, n, len(to_copy)
                    ))                    
                os.makedirs(os.path.dirname(target_file), exist_ok=True)
                if hardlink:
                    os.link(filename, target_file)
                else:
                    shutil.copyfile(filename, target_file)

            result_file = os.path.join(collection_dir, target_collection_name) + ".csv"
            old_result = {}
            if os.path.exists(result_file):
                old_result = load_csv(result_file)
            for target_entry in old_result:
                if target_entry not in result:
                    target_file = os.path.join(target_collection_dir, target_entry)
                    if os.path.exists(target_file):
                        os.remove(target_file)
            write_csv(result, result_file)
            collections[target_collection_name] = {
                "copied": True,
            }
        
        elif name == "build_download_index":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]
            if source_collection.get("default"):
                entries = inode_to_entry.values()
            elif source_collection.get("copied"):
                raise NotImplementedError
            else:
                msg = 'Action "build_download_index": source collection "{}" must be default or copied'
                err(msg.format(source_collection_name))
            
            download_index = {}
            urls = action["urls"]
            regex = re.compile(action["source_file"])
            for entry in entries:
                unnamed_capturing_groups = regex.match(entry).groups()
                named_capturing_groups = regex.match(entry).groupdict()
                target_urls = []
                for urldict in urls:
                    if isinstance(urldict, str):
                        urldict = {"url": urldict}
                    target = {}
                    for attr in "format", "celltype":
                        if attr in urldict:
                            target[attr] = urldict[attr]
                    target["url"] = urldict["url"].format(
                        *unnamed_capturing_groups,
                        **named_capturing_groups
                    )
                    target_urls.append(target)
                download_index[entry] = target_urls
            download_index_dir = get_subdir("download_indices")
            download_index_file = os.path.join(download_index_dir, source_collection_name) + ".json"
            report("Write download index to {}".format(download_index_file))
            write_json(download_index, download_index_file)
        
        elif name == "dataset":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]
            dataset_dir = get_subdir("datasets")
            dataset_file = os.path.join(dataset_dir, source_collection_name) + ".json"
            if os.path.exists(dataset_file):
                old_dataset = json.load(open(dataset_file))
                old_dataset_checksum = get_hash(get_plain_buffer(old_dataset), hex=True)
                old_dataset_inode = str(os.stat(dataset_file).st_ino)
            else:
                old_dataset_checksum = None
            dataset = {}
            if source_collection.get("default"):
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    dataset[entry] = checksum        
            elif source_collection.get("copied"):
                raise NotImplementedError
            else:
                msg = 'Action "dataset": source collection "{}" must be default or copied'
                err(msg.format(source_collection_name))

            dataset_checksum = get_hash(get_plain_buffer(dataset), hex=True)
            print("Computed dataset for {}: deep buffer checksum {}".format(source_collection_name, dataset_checksum))
            if old_dataset_checksum is not None and dataset_checksum != old_dataset_checksum:
                remove_stale_buffer_hardlink(old_dataset_checksum, old_dataset_inode)
            report("Write dataset deep buffer to {}".format(dataset_file))
            write_json(dataset, dataset_file)
            intern_buffer(dataset_checksum, dataset_file, hardlink=True)
        
        elif name == "deepcell":
            raise NotImplementedError(action)                