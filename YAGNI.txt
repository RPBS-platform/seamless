Ideas that are superfluous. Keep them here, just in case...

- Address GLstore memory leak: stores may not be freed (is this still so post-0.1 ??)
(Use Blocks instead)

- implement successors (YAGNI? now that the low-level is subordinate, better rip them)

- ATC chains with folding/unfolding (YAGNI?)

- Dynamic connection layers: a special macro that has one or more contexts as input (among other inputs), which must be (grand)children
   Like static layers, they are tied to a macro or toplevel context
   They take as input any children of the context (cells or child contexts);
   Builds connections within/between those children.
   They can never build new cells, use the help of a macro for that.
   The result layer consists of a set of pairs (connection + callback that removes that connection),
   Dynamic connection layers will always be evaluated after any other forward update.
   Therefore, if you have A => macro => B, A + B => dynamic layer, you can assume that the macro
    will respond first to changes in A, so that B will reflect the new A.

    - Report cells (JSON cells, can become structured if directed from the mid-level).
      Status dict becomes a a Report cell.
    - Log cell: text cell to which an observer can be attached that receives new entries)
       The result of translation, caching, macros, etc.; generalized log API.
       Even transformers and reactors may be declared as having a log output, and various loglevels
        (transformer.py will already send low-priority log messages about receiving events etc.)

Event streams
=============
Event streams receive event values, or a "undo" signal, which means that all previous
values are invalid. Event streams may send back a "send again" signal, which means
that they want again all values that were previously sent to them. (This is for example if a transformer adds 5 to an event stream; if 5 is changed to 6, either in a input cell or by a change in the source code, the transformer will send a "send again" signal upstream).
Reactors may choose to cache events so to avoid sending this signal when one of their other inputs changes.
Finally, there are the "initialize" and "disconnect" signals.
Event stream inputpins are in a state where they accept a new value, or they don't. Force-feeding is possible, in that case the values are buffered up by seamless itself.
Workers must always declare explicitly a pin as event stream.
Transformers don't need any change in their code. However, if one or more of their inputs is an event stream, so must be their output (vice versa is not required, but if all inputs are cells, a transformer that has an event stream as output must return a list, which is force-fed into the event stream after sending an "initialize" signal).
Reactors must push/pull new event stream values in an explicit API:
  - blocking waits for it
  - non-blocking essentially sets to event stream input pin to "accept input" (input) or force-feeds (output)
  - By default, input is blocking while output is non-blocking
Event streams can never be authoritative, they must depend on a worker.
NOTE: The trouble won't be the implementation... but event streams have serious consequences for caching!
...
(In relation to ATTRACT mainloop)
After event streams, the mainloop reactor will be superfluous
 and can be built as a simple context of event stream cell A-D plus connections,
 with an internal counter (event streams must provide this as a Report cell!) to make the B=>C connection conditional.
Equilibrium contexts with an event stream as input will automatically give one as output, and send the proper "undo" signal
 when they change, resetting the stream.

Compressed blocks
=================
If the mode is "compress", the block is tiled *physically* over "compress" tiles, but *logically* over "length" tiles.
For example, take a block of a million elements, with "length" as 1000 but "compress" as 10. This means that physically,
 the block is divided into 10 tiles of 100k elements, but *logically*, there exists 1000 tiles of 100k elements, for a
 logical array size of 100 million elements. Of those 1000 tiles, only 10 may be physically held in memory.
....
If only "split" mode is used, tiling is just a way to process parts of the buffer independently.
However, if "compress" mode is used, the situation becomes more complicated.
Like event streams, a tiling outchannel then holds a lock, which is released when all (but not really all, see below)
of its downstream  dependencies are done with it. The number of locks is equal to "compress", the number of physical tiles.

Runtime caching
===============
New cell type: cache cell.
All (low-level) workers (transformers/reactors/macros) may take (up to) one pin of type "cache" as inputpin (not editpin).
If they take such a pin, they may raise a CacheError. This will clear the cache, and put the worker
in "CacheError" state. CacheErrors are meant to detect *stale* caches: workers are forbidden to raise CacheError if the cache is empty. (UPDATE: multiple cache inputs, but CacheError clears all of them)
It is understood that the content of cache cells *do not influence the result whatsoever*.
  - Dirty cache cells do not trigger re-evaluation of downstream workers, unless those are in "CacheError" state.
  - Cache cells alone may have multiple sources of authority, i.e. multiple outputpins/editpins connecting to them.
    (UPDATE: better not do this...)
Special transformers are "caching transformers", they have a "cache" cell as output.
Caching transformers are triggered when their input changes *or* their cache output is cleared.
Caching transformers alone can have multiple cache inputs, and have an API to clear them individually.
Cache clearing counts as a signal in seamless, which means that the subsequent triggering of the caching transformers
has the highest evaluation priority.
Workers in "CacheError" state are re-evaluated whenever their cache input changes.
If the cache input stays cleared, and the context is in equilibrium, they are nevertheless evaluated with
empty cache (and the CacheError state is removed).
UPDATE: It is nice to co-opt this mechanism so that transformers can store partial results, and continue.
This can be done using a macro around a transformer (can be triggered using mid-level syntax).
The transformer has a cache inputpin that is connected from a cache cell. The same cache cell is also connected to
the secondary outputpin (result_preliminary), this must be allowed. (UPDATE: or use a special cache edit pin? this example
won't need a cache transformer then)
The macro generates a cache transformer that clears the cache cell whenever any of the inputs (including the code) changes.
The transformer code must be able to analyze the contents of the preliminary results in cache and act accordingly.
The cache cell will be marked as being serialized upon save (but not mounted).
UPDATE: Maybe re-think this a bit... maybe mark cells as "cache", and allow transformer edit pins only to "cache" cells.
 For the rest, rely on concretification signals to compute caches just in time (PIN.cache.value could trigger concretify
 in a blocking manner, no more CacheError foo?).

Network services (high level)
=============================
UPDATE: slightly outdated. Will be mostly implemented as high-level macros.
NOTE: these are to implement foreign (non-Seamless) web services. Seamless web services have better ways to communicate
 (see below).
Seamless will have the core concept of *network services*.
Seamless has a universal network service handler: it receives a protocol (REST, websocket, etc.),
a URI, a port, and JSON data. Data is sent, the result is returned.
Registering a network service takes the following parameters:
- type: can be "transformer", "reactor" or "macro"
- code: the code string that is serviced. The code string contains the source code of the transformer or macro. In case of "reactor", a dict of the three code strings. Also contains the language of the source code (default: Python)
- parameter pin dict. Must match the pin parameters of the transformer/reactor (equivalent for macro).
- adapter: code string (+ language) of the function that converts the input into parameters for the handler.
- schema_adapter: same, but receives the schema of the input instead (and also the code).
- handler_parameters: hard-coded parameters for the handler
- post_adapter: Another code string (+ language) to convert the handler results to pins. Optional for transformers/macros.
Adapter, schema_adapter must each return a dict, or raise an ServiceException if they decide that the service is not suitable based on the schema/the data.
The handler_parameters dict is updated by the schema_adapter result dict, then updated by the adapter result
dict, then sent to the handler. The result of the handler is passed to the post_adapter.
It is possible to set in the "evaluation" dict some flag that forces service evaluation: however, this
should not influence the result! The local code must be correct!

On top of this, network service macros can be implemented, that take slightly different parameters.
For example: named REST service handler, taking the following parameters:
- name: name of the REST service
- code: transformer code to be evaluated locally if the REST service is not found

Example: raw network service handler. Receives a URL + port + data. Sends data, returns the result.
Another: raw REST service handler. Same, but HTTP REST protocol.
Another: named network service handler. Receives not a URL but the *name* of a network service. Relies on a registry to convert this name into some kind of network call (could also be docker).
Remember that seamless assumes that the result of a computation is constant, regardless of service. So changing a service registry will not automatically re-evaluate the computation!
Now, the adapters also receive the "evaluation" parameter, so this can be forwarded to the handler!
Likewise, the adapters may combine this with its own "evaluation" analysis, based on what they receive.
For example, you may inform the ATTRACT grid computation service that your are planning to send 1 trillion
docking energy evaluations to the ATTRACT grid. A dumb ATTRACT grid service would build the grid on one machine,
and return some kind of session ID. This session ID is stored as cache in both the input and the result
"evaluation".
The session ID in the result "evaluation" can then be used to query the ATTRACT service with structures

The web publisher channels
===========================
Seamless will include a pocket web publisher. Each publisher can be made available on the Seamless router as a pair of RPCs: one to submit a web page under a path (providing some kind of
  authorization) and another to retrieve the page
- Static publisher: takes an HTML template and a host channel ID. The host channel ID is substituted into the HTML. The HTML is supposed to contact the channel via WAMP. If the channel comes
  from seamless, there will be a seamless collaborative sync protocol behind it: dynamic_html, or direct cell synchronization
  The static publisher does not take any arguments
- Dynamic publisher: takes an HTML template and a factory channel. The factory channel is invoked (without arguments) and returns a host channel ID.
A web server can serve the static publisher directly. The dynamic publisher should be accessible in two ways:
- Launcher: web page ID is in the request, no further arguments. Invokes the factory, redirects to the Retriever, with the host channel ID as parameter
- Retriever: web page ID in the request, host channel ID as parameter. Takes the HTML template, fills in the host channel and returns the HTML.
  As long as the host channel is open, the Retriever link will be universally accessible (no private browser connections).



  Macros
  High-level macros use the high-level API and generate a high-level context.
  (UPDATE: or a mid-level graph structure!)
  Low-level macros use the low-level (direct-mode) API and generate a low-level context.
    They can be embedded in the mid-level graph (specifying Python as the language)
  Some high-level contexts can be configured as *libraries*:
  - To be also available to any macro inside any child context of their parent, if the macro asks for it
  - To be also available as low-level Silk struct, inside low-level macros
  Such contexts will replace the standard library and registrars.
  UPDATE: libraries will be a low level concept now.
  All macros have four parts:
  - execution code
  - loading code (imports)
  - library requirements (see above)
  - configuration (language, what happens when you assign to it)
  Symlinks are also very important to tie a cell to a library cell. UPDATE: no longer true (LibCell instead)

- Organize cells into arrays (probably at high-level only)

- Other mount backends (databases)
  As a variation, an option for cells to have no .\_val, i.e. .value always pulls
   the value from the backend (it is assumed not to have changed!)
  Multiple mounts should be supported. Concept of "mount namespace", default ones: "file", "uri"
   Namespace is required, but mount identifier (file name, URL, ...) is optional (checksum might suffice)
   Mounts maybe read and/or write, but read may also be lazy: workers have to actively demand the value from
   the mount (this will not be kept in memory afterwards, caching has to be done by the mounter)
   UPDATE: maybe generic checksum value server will be enough?

 - Preliminary outputpins (in transformers [as secondary output] and in reactors)
 - Preliminary inputpins (pins that accept preliminary values). Right now, all inputpins are preliminary!
 - Equilibrium contexts (see below)
 - finalize the design of mid-level graph syntax.
   - Include old 0.1 resources, or make this high-level only?
   - Save high-level syntax as mid-level only, or separately?

 Concretification
 ================
 "Concretification" is a feature that cells may not need to store their full value.
 It involves sending two signals upstream, i.e. a cell's input pin or inchannels
 that requests something from the upstream worker or cell.
 1. send my hash/checksum
 2. (re-)send again my value
 Applications of this are in:
 - blocks (with compression; see below),
 - backend caching (a cell may just store its hash and demand its value when needed from
    a cell server; if the server fails, it may send a signal 2.)
 - lazy evaluation (in lazy mode, transformers evaluate only upon receiving a signal 2.
 Reactors and macros send a signal 2. to each of their input cells
 3. Release. In case of lazy cells, signal 2. is non-blocking; the manager is notified when
  the result is ready. The manager then forwards the results, and sends back a "release" signal,
  meaning that the value may be freed from memory. If this indeed happens or not, depends on
  the configuration (although compressed blocks always free).
  The checksum of the cell always kept. The value may still be in a checksum cell server.
  If the value cannot be retrieved, a signal 2. is sent upstream.
  (Non-lazy cells may also rely on such a server, but in their case, it is an error if the server fails.)

 Lazy evaluation
 ===============
 Lazy cells are cells that are marked as such. They must be dependent on a
  transformer.
 Reactors immediately concretify all of their lazy inputs, and cannot have a
  lazy output/edit.
 However, transformers that output lazy cells automatically become lazy.
 They concretify their inputs only when its output cell becomes concretified.

 An important advantage of lazy cells is that there are stronger timing guarantees.
 Signal 2. (invalidate the value) is propagated forward *immediately* by Seamless
  towards downstream dependencies (stopping transformers in their tracks), before
  sending any values to the workers.
 Example: dependency graph A=>B, A=>C, B+C=>D. If A is updated to A*,
  B will become updated to B* and C to C*.
  Eventually, D will be evaluated from B*+C*, but the intermediate computations
  B*+C or B+C* are both possible as "glitches". However, if B and C are lazy, then
  an update for A will immediately invalidate them, and the glitches disappear.
 From the Python shell, lazy cells have a method that concretifies in a non-blocking manner.
 In addition, they have a method that concretifies in a blocking manner
  and then returns a result.
 Finally, they have a channel that sends concretification signals,
  to which Python can subscribe callbacks (like an observer or Hive push antenna)

 In addition to LazyCell, there is LazyCallback, which emulates LazyCell.
 However, a LazyCallback wraps a zero-argument Python callback (blocking or non-blocking)
  into a cell value. This callback is triggered upon concretification.
 With cffi, the callback can come from another language, like C or even Haskell.
 Note that callback foo cannot be serialized, it is the responsibility of
  Python (or whatever host language) to bind the callbacks after graph instantiation.


Workers and memory
==================
By default, workers keep all of their values buffered. This may eat up a lot of
memory resources. Pins may be configured to be on-demand instead. In that case,
seamless will send only the checksums, and the worker will then have to ask for
the value.
In case of transformers/macros, this is a single request for all of the values.
For reactors, the request is actually made when the pin value is demanded.
This request is blocking, so it should probably only be used for async reactors.


Equilibrium contexts
====================
Transformers are guaranteed not to send anything (be it cell values or events) on their primary output until execution has finished (which means they are in equilibrium).
In addition, transformers are guaranteed not to accept any events while not in equilibrium. If there are any,
 the transformer computation is actually canceled.
This is obviously not so for reactors, and it is also not so for contexts that contain reactors (or multiple transformers that are not arranged linearly) connected to exported outputs of the context.
It is possible to declare contexts as "equilibrium contexts". In that case, they have the same guarantees as transformers have: sending cell updates or events to the outside world is delayed until equilibrium is reached, and so is the acceptance of new events. This allows contexts to perform atomic computations, reducing the number of glitches.
It is possible to declare some of the outputs as "secondary", which means that they escape this guarantee (for example, for logging purposes).
"Events to the outside world" is only restricted if it goes through exported cells and pins. Traffic through
non-exported objects is not restricted.

Block allocation
================
Block allocation can happen in two ways:
- Via an Allocator (new low-level construct). The block descriptor must be given to the allocator constructor.
  The Allocator provides an allocator output pin, which gives both the block descriptor and the allocated buffer.
- Via a StructuredCell, in Silk mode. The path of every allocation must be declared in "allocators", which works
  similar to "outchannels": each path becomes an allocator output pin.
  From the shell, the allocation path value is readable (resulting in the block value, copied to a Numpy array)
  but not writable. No inchannel or outchannel may be a superpath of the allocation path (at the high level, this
   must be checked during assignment, before translation).
  The StructuredCell must have all block descriptors declared during construction, including namespace.
  However, for high-level cells, the block descriptor is read from the .schema. In any case, if there is a schema
   entry, the StructuredCell will check that it is compatible with the block descriptor.
  When the OverlayMonitor (or its subclass) is created, a buffer is created (in the appropriate namespace)
  using the block descriptor, and when activated, both are sent over the allocator output pin.

  Application to ATTRACT: the mainloop reactor
  ============================================
  Because of the cylic dependency, an energy minimization loop is not a good fit for Seamless. But it can be done.
  There must be an ATTRACT mainloop reactor with editpins A and C, and inputpins B and D.
  Pin A contains the DOFs: upon start-up, it is copied from the initial DOFs.
  It is assumed that a "scorer" context listens to A and gives results on B, which is the energy and DOF gradients.
  To activate the scorer, the mainloop reactor sets A, then equilibrates A and B, then reads B.
  Pin C also contains the energy and DOF gradients, and D also contains the DOFs
  It is assumed that a "minimizer" context listens to C and then gives results on D.
  To activate the minimizer, the mainloop reactor sets C from B, then equilibrates C and D, then reads D.
  The scorer can then be re-activated by setting A from D.
  The minimizer and scorer contexts must be equilibrium contexts.
  "Equilibrating" is done by a having a signal that fires when a context reaches equilibrium.
   Use "on_equilibrate" hook of root managers (adapt for non-root).
  The ATTRACT mainloop reactor will have two signal inputpins for this (from scorer and minimizer) and an internal
   variable that maintains which signal is to be listened for.
  A and C are cached together with the number of minimization steps that have been performed. In this way:
  - The mainloop context can be saved in mid-evaluation and resumed later
  - The number of steps can be increased and only the additional steps are computed.
  Equilibrium contexts should also have a Report cell that reflects the checksum value of all exported cells and pins
  that are "input".
  Exported cells are "input" if they are authoritative within the cell. Exported pins are "input" if they are inputpins.
  The mainloop reactor will listen for these checksum Report cells; when they change, the whole computation must be restarted.
  When porting ATTRACT, the minimizer and scorer should be in slash.
